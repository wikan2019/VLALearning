# 附录 B：双视觉编码器详解——DINOv2 + SigLIP/CLIP

> **前置知识**：[19. VLA 架构](./19_VLA_Architecture.md)（视觉编码器章节）、[21. OpenVLA](./21_OpenVLA.md)

---

## 1. 为什么需要双视觉编码器？

机器人操作需要同时具备两种截然不同的视觉能力：

```
任务: "把红色杯子放到微波炉里"

需要的视觉能力:

  能力 1: 语义理解 (Semantic Understanding)
    → "什么是杯子？什么是微波炉？红色是什么颜色？"
    → 需要和语言对齐的视觉特征

  能力 2: 空间定位 (Spatial Localization)
    → "杯子在桌面的哪个精确位置？微波炉门的把手在哪？"
    → 需要精确的像素级空间特征
```

**问题**：没有一个单一的视觉编码器能同时把这两件事都做到最好。

```
┌─────────────────────────────────────────────────────────────────┐
│ 单编码器的困境                                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  CLIP/SigLIP (对比学习):                                         │
│    ✅ "这是杯子" "这是微波炉" → 语义识别强                       │
│    ❌ "杯子的精确像素坐标是 (142, 287)" → 空间信息弱             │
│                                                                  │
│  DINOv2 (自监督学习):                                            │
│    ✅ "物体边界在第 142 行，表面纹理呈弧形" → 空间细节强          │
│    ❌ "这个东西叫杯子，和文字'cup'对应" → 语义对齐弱             │
│                                                                  │
│  解决方案: 两个都用！                                            │
│    DINOv2 (空间) + SigLIP (语义) = 看得准 + 认得清               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. CLIP：对比学习视觉编码器

### (a) 核心思想

CLIP (Contrastive Language-Image Pre-training, OpenAI, 2021) 的核心思想：**用互联网上海量的图文对，训练一个视觉编码器和文本编码器，使它们的表示在语义上对齐**。

```
训练数据: 4 亿个 (图像, 文本) 对，来自互联网

  ("一只猫坐在沙发上", 🖼️猫图)   → 匹配 ✅
  ("一辆红色跑车", 🖼️猫图)       → 不匹配 ❌
```

### (b) 架构

```
┌──────────────┐          ┌──────────────┐
│  Image       │          │  Text        │
│  Encoder     │          │  Encoder     │
│  (ViT)       │          │  (Transformer)│
│              │          │              │
│  🖼️ → f_img  │          │  📝 → f_txt  │
└──────┬───────┘          └──────┬───────┘
       │                         │
       ▼                         ▼
   f_img ∈ R^d              f_txt ∈ R^d
       │                         │
       └─────────┬───────────────┘
                 │
          余弦相似度 sim(f_img, f_txt)
```

### (c) 训练损失：InfoNCE (Softmax 对比损失)

CLIP 使用**对称的 Softmax 对比损失**：

```
给定 batch 中 N 个 (图像, 文本) 对:

  相似度矩阵 S ∈ R^(N×N):
    S[i,j] = τ · cos(f_img_i, f_txt_j)     ← τ 是可学习的温度参数

  图像→文本方向的损失:
    L_i2t = -(1/N) Σᵢ log( exp(S[i,i]) / Σⱼ exp(S[i,j]) )
                                  ↑ 正样本      ↑ 所有样本(正+负)

  文本→图像方向的损失:
    L_t2i = -(1/N) Σⱼ log( exp(S[j,j]) / Σᵢ exp(S[i,j]) )

  总损失:
    L_CLIP = (L_i2t + L_t2i) / 2
```

**可视化**：

```
相似度矩阵 (batch_size = 4):

              文本₁   文本₂   文本₃   文本₄
        ┌──────┬──────┬──────┬──────┐
图像₁   │ 0.95 │ 0.02 │ 0.01 │ 0.03 │  ← 对角线应该最大 (匹配对)
        ├──────┼──────┼──────┼──────┤
图像₂   │ 0.01 │ 0.92 │ 0.05 │ 0.02 │
        ├──────┼──────┼──────┼──────┤
图像₃   │ 0.03 │ 0.01 │ 0.94 │ 0.02 │
        ├──────┼──────┼──────┼──────┤
图像₄   │ 0.02 │ 0.04 │ 0.01 │ 0.93 │
        └──────┴──────┴──────┴──────┘

训练目标: 最大化对角线 (正样本对), 最小化非对角线 (负样本对)
```

### (d) CLIP 的特点

| 方面 | 说明 |
| :--- | :--- |
| **预训练数据** | 4 亿互联网图文对 (WebImageText) |
| **训练信号** | 图文对齐（对比学习）→ 特征空间中图像和文本的语义对齐 |
| **强项** | 语义理解强，zero-shot 分类、图文检索 |
| **弱项** | 空间细节丢失，因为 [CLS] token 压缩了整张图的语义，丢弃了像素级信息 |
| **对 VLA 的意义** | 理解"这是什么物体"、"指令说的是哪个东西" |

### (e) CLIP 的关键问题：Softmax 的瓶颈

```
Softmax 对比损失的问题:

  L_i2t = -log( exp(S[i,i]) / Σⱼ exp(S[i,j]) )
                                ↑ 需要对整个 batch 做归一化！

  问题 1: Batch Size 依赖
    softmax 的分母需要所有负样本的相似度
    → batch 越大，负样本越多，训练越好
    → CLIP 原论文用 batch_size = 32768！
    → 需要大量 GPU 并行

  问题 2: 数值稳定性
    softmax 计算时需要减去最大值防止溢出
    → 分布式训练中需要跨 GPU 同步最大值
    → 通信开销大

  问题 3: 隐式假设
    每个图像只对应一个正确文本 (batch 内其他都是负样本)
    → 如果 batch 内有两张猫图，它们的文本会被当成负样本
    → 产生 false negative 问题
```

这些问题促使了 SigLIP 的诞生。

---

## 3. SigLIP：Sigmoid 对比学习

### (a) 核心改进

SigLIP (Sigmoid Loss for Language-Image Pre-training, Google, 2023) 的核心改进：**用 Sigmoid 替代 Softmax**——将全局归一化的对比学习变成**逐对独立**的二分类。

```
CLIP (Softmax):  "在 N 个候选中，哪个文本最匹配这张图？"  → N 类分类
SigLIP (Sigmoid): "这张图和这段文本匹配吗？是/否"           → 二分类 × N²
```

### (b) 训练损失：Sigmoid 对比损失

```
SigLIP 损失:

  对于每一对 (图像ᵢ, 文本ⱼ):

    z_ij = -1 · (2·y_ij - 1) · (τ · cos(f_img_i, f_txt_j) + b)
           ↑ 符号翻转        ↑ 标签(匹配=1, 不匹配=0)  ↑ 偏置项

    L_SigLIP = (1/N²) Σᵢ Σⱼ log(1 + exp(z_ij))

  其中:
    y_ij = 1 当 i=j (匹配对)，y_ij = 0 当 i≠j (不匹配对)
    τ = 可学习温度参数
    b = 可学习偏置项

  等价形式 (更直观):
    对匹配对 (i=j):  L = -log σ(τ·sim + b)     ← 希望相似度高
    对不匹配对 (i≠j): L = -log σ(-τ·sim - b)   ← 希望相似度低
```

### (c) SigLIP vs CLIP 的关键差异

```
┌──────────────────────────────────────────────────────────────────┐
│                     CLIP (Softmax)                                │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Softmax: P(j|i) = exp(S[i,j]) / Σ_k exp(S[i,k])               │
│                                     ↑ 全局归一化                  │
│  → 需要看到 batch 内所有样本                                     │
│  → 跨 GPU 通信量大                                               │
│  → 隐式假设: 每张图只有一个正确文本                              │
│  → batch 内的 false negatives 有害                               │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────┐
│                     SigLIP (Sigmoid)                              │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Sigmoid: P(match|i,j) = σ(τ·sim(i,j) + b)                     │
│                           ↑ 逐对独立判断                          │
│  → 每对独立计算，无需全局归一化                                   │
│  → 分布式训练更简单                                               │
│  → 不假设唯一正确文本                                            │
│  → 对 false negatives 更鲁棒                                     │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

| 对比维度 | CLIP (Softmax) | SigLIP (Sigmoid) |
| :--- | :--- | :--- |
| **损失函数** | N 类 softmax 交叉熵 | 逐对 sigmoid 二元交叉熵 |
| **归一化范围** | 全局（整个 batch） | 局部（每对独立） |
| **最优 batch size** | 极大 (32K+) | 适中即可 (16K-32K) |
| **分布式训练** | 需要跨 GPU 同步 | 各 GPU 独立计算 |
| **false negative** | 敏感（batch 内同类被当负样本） | 鲁棒 |
| **训练效率** | 4 TPUv4 chips 难以训练 | 4 TPUv4 chips 可达 84.5% IN zero-shot |
| **ImageNet zero-shot** | ~76% (ViT-B/16) | ~84.5% (ViT-B/16, 更高效) |
| **语义对齐质量** | 优秀 | 优秀（略优于同等计算量的 CLIP） |

### (d) 为什么 VLA 更倾向于 SigLIP？

```
VLA 选择 SigLIP 的原因:

  1. 训练效率更高 → 更容易在有限资源下训练/微调 VLA
  2. 语义对齐质量相当甚至更好
  3. 更好的 scaling 特性 → 适合 VLA 的大规模预训练
  4. SigLIP 是 PaliGemma (Google) 的视觉编码器
     → π0 使用 PaliGemma 作为 VLM 骨干
     → OpenVLA 的 Prismatic VLM 也使用 SigLIP
```

---

## 4. DINOv2：自监督视觉编码器

### (a) 核心思想

DINOv2 (Meta, 2023) 与 CLIP/SigLIP 完全不同——它**不使用任何文本标注**，纯粹通过图像的**自监督学习**来获得视觉表示。

```
CLIP/SigLIP:  图像 + 文本 → 对比学习 → 语义对齐的表示
DINOv2:       只用图像    → 自蒸馏    → 空间丰富的表示
```

### (b) 自蒸馏架构 (Self-Distillation)

DINOv2 使用**师生网络 (Teacher-Student)** 框架，学生和教师具有相同的架构：

```
┌────────────────────────────────────────────────────────────────┐
│                    DINOv2 自蒸馏框架                             │
├────────────────────────────────────────────────────────────────┤
│                                                                 │
│   原始图像 🖼️                                                   │
│      │                                                          │
│      ├──→ 全局裁剪 (224×224) × 2  ─→  教师 ViT (θ_t)          │
│      │    (覆盖 >50% 面积)              ↓                       │
│      │                              P_t = softmax(h_t / τ_t)   │
│      │                                                          │
│      └──→ 全局裁剪 × 2 + 局部裁剪 (96×96) × N  ─→ 学生 ViT (θ_s) │
│           (局部裁剪覆盖 <50% 面积)                  ↓            │
│                                                  P_s = softmax(h_s / τ_s) │
│                                                                 │
│   训练信号:                                                     │
│     教师看大图，学生看大图和小图                                 │
│     → 学生必须从局部信息推断出全局语义                           │
│     → 学到的特征同时包含局部细节和全局结构                       │
│                                                                 │
│   教师更新: θ_t ← m·θ_t + (1-m)·θ_s    (EMA，不用梯度!)       │
│            m: 0.996 → 1.0 (余弦调度)                            │
│                                                                 │
│   只有学生通过梯度下降更新 → 教师是学生的"平滑版本"             │
│                                                                 │
└────────────────────────────────────────────────────────────────┘
```

### (c) DINOv2 的三重训练损失

DINOv2 结合了三个互补的训练目标：

```
L_DINOv2 = L_DINO + L_iBOT + λ · L_KoLeo
           ↑ 图像级      ↑ Patch级     ↑ 正则化
           (全局语义)    (局部空间)     (特征均匀性)
```

**损失 1: L_DINO（图像级自蒸馏）**

```
L_DINO: 对 [CLS] token 做师生蒸馏

  教师输出: P_t^cls = softmax((h_t^cls - c) / τ_t)
                               ↑ centering    ↑ 低温 (锐化)
  学生输出: P_s^cls = softmax(h_s^cls / τ_s)
                                       ↑ 高温 (软化)

  L_DINO = -Σ_k P_t^cls[k] · log P_s^cls[k]     (交叉熵)

  作用: 让学生学到教师的全局图像级语义表示
        [CLS] token 编码了"这张图整体是什么"
```

**损失 2: L_iBOT（Patch 级 Masked Image Modeling）**

```
L_iBOT: 对被遮蔽的 Patch token 做师生蒸馏

  步骤:
    ① 随机遮蔽学生输入的部分 Patch (掩码率 ~40%)
    ② 教师看完整图像，学生看被遮蔽的图像
    ③ 学生必须预测被遮蔽位置的教师输出

  L_iBOT = -Σ_{i∈masked} Σ_k P_t^patch_i[k] · log P_s^patch_i[k]

  作用: 让每个 Patch token 学到精确的局部空间信息
        "这个 patch 位置有什么？"
        → 这就是 DINOv2 空间能力强的核心原因！
```

**损失 3: L_KoLeo（KoLeo 正则化）**

```
L_KoLeo: 鼓励特征在超球面上均匀分布

  L_KoLeo = -(1/N) Σᵢ log(min_{j≠i} ||f_i - f_j||)

  作用: 防止特征坍缩 (collapse)，保持特征空间的多样性
```

### (d) DINOv2 训练细节

| 方面 | 说明 |
| :--- | :--- |
| **训练数据** | LVD-142M：1.42 亿张精心筛选的图像（无文本标注） |
| **数据筛选** | 自动化管道：从海量网络图片中筛选多样、高质量的子集 |
| **模型规模** | 先训练 ViT-g/14 (1.1B)，再蒸馏为 ViT-S/B/L |
| **训练技巧** | FlashAttention、混合精度、FSDP 分布式训练 |
| **不需要标签** | 完全自监督，不依赖人工标注或文本描述 |
| **训练轮数** | 约 625K iterations (ViT-g on 142M images) |

### (e) DINOv2 的特征可视化

```
DINOv2 学到的 Patch 特征 (PCA 可视化):

原图:                    DINOv2 Patch 特征:
┌──────────────┐         ┌──────────────┐
│   🐕🐈       │         │ ████ ████    │  ← 物体轮廓清晰分离
│              │         │ ████ ████    │
│  桌子        │         │              │  ← 背景统一
│              │         │ ████████████ │  ← 桌面连续
└──────────────┘         └──────────────┘

关键观察:
  ① 同一物体的 Patch 特征高度一致 (语义分割效果)
  ② 物体边界清晰（精确的空间分辨）
  ③ 不同物体的特征明显分离

对比 CLIP 的 Patch 特征:
  → CLIP 的 Patch 特征模糊，边界不清晰
  → CLIP 更关注"这是什么"而非"在哪里"
```

---

## 5. DINOv2 vs SigLIP/CLIP：核心差异

### (a) 训练范式对比

```
┌─────────────────────────────────────────────────────────────┐
│                CLIP / SigLIP                                 │
│                (对比学习 + 文本监督)                          │
│                                                              │
│  训练信号: 图像 ←对齐→ 文本                                  │
│            "猫" ←→ 🖼️猫                                      │
│                                                              │
│  学到的:  与语言对齐的语义空间                                │
│           "猫"和猫图在同一个位置                              │
│           → 擅长: 语义理解、指令跟随                          │
│                                                              │
│  丢失的:  精确的空间位置信息                                  │
│           (对比学习压缩了空间细节，只保留语义)                │
│                                                              │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                DINOv2                                        │
│                (自监督 + 自蒸馏)                              │
│                                                              │
│  训练信号: 图像 ←自蒸馏→ 图像 (不同视角/裁剪)               │
│            小裁剪 → 预测 → 全局裁剪                          │
│            遮蔽patch → 预测 → 完整patch (iBOT)               │
│                                                              │
│  学到的:  丰富的视觉空间结构                                 │
│           每个 patch 有精确的位置和纹理信息                   │
│           → 擅长: 空间定位、物体边界、密集预测                │
│                                                              │
│  丢失的:  与语言的对齐                                       │
│           (从未见过文本，不知道"猫"这个词)                    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### (b) 特征空间对比

```
同一张图 (桌上有红色杯子 + 蓝色碗):

SigLIP/CLIP 特征 ([CLS] token):
  "红色杯子和蓝色碗在桌上" → 一个高维向量
  → 可以和文本 "red cup on table" 匹配
  → 但很难从这个向量中精确还原杯子在第几行第几列

DINOv2 特征 (Patch tokens):
  Patch (3,5): 红色、圆柱形表面、杯子的一部分
  Patch (3,6): 红色、圆柱形边缘、杯子右侧
  Patch (7,4): 蓝色、弧形表面、碗的上沿
  → 每个 Patch 都包含精确的空间和外观信息
  → 但不知道 "红色杯子" 这个概念和文字的对应
```

### (c) 量化对比表

| 维度 | CLIP | SigLIP | DINOv2 |
| :--- | :--- | :--- | :--- |
| **训练数据** | 4 亿图文对 | 数十亿图文对 | 1.42 亿纯图像 |
| **训练方式** | Softmax 对比学习 | Sigmoid 对比学习 | 自蒸馏 + iBOT |
| **需要文本标注** | ✅ | ✅ | ❌ |
| **语义理解** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ |
| **空间定位** | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| **语言对齐** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ❌ |
| **密集预测** | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| **ImageNet 线性分类** | ~80% | ~84% | ~86.5% (ViT-g) |
| **分割/检测** | 一般 | 一般 | 优秀 |
| **zero-shot 分类** | 优秀 | 优秀 | 不支持 (无文本编码器) |

### (d) 直觉类比

```
SigLIP/CLIP ≈ 一个"博学的翻译官"
  → 懂得图像和语言的对应关系
  → 你说 "cup"，它能指出图中的杯子
  → 但描述精度有限: "杯子大概在图的右半部分"

DINOv2 ≈ 一个"精密的测量仪"
  → 能精确测量图像中每个像素的特征
  → 物体边界精确到像素级
  → 但不懂人类语言: "你说 cup？我不认识这个词"

组合使用:
  翻译官说: "指令要抓杯子，杯子在右半部分"
  测量仪说: "右半部分有个物体，中心在 (156, 234)，边界是..."
  → 机械臂精确抓取 ✅
```

---

## 6. 双编码器融合：Prismatic VLM 架构

### (a) OpenVLA 的融合方法

OpenVLA 基于 **Prismatic VLM** 框架，采用**特征拼接 (Feature Concatenation)** 的方式融合两个编码器：

```
┌──────────────────────────────────────────────────────────────────────┐
│                    Prismatic VLM 双编码器融合                         │
├──────────────────────────────────────────────────────────────────────┤
│                                                                       │
│   输入图像 (224×224)                                                  │
│      │                                                                │
│      ├─────────────────┐                                              │
│      │                 │                                              │
│      ▼                 ▼                                              │
│  ┌──────────┐    ┌──────────┐                                        │
│  │ SigLIP   │    │ DINOv2   │                                        │
│  │ ViT-SO   │    │ ViT-L/14 │                                        │
│  │ (400M)   │    │ (304M)   │                                        │
│  └────┬─────┘    └────┬─────┘                                        │
│       │               │                                               │
│       ▼               ▼                                               │
│  [v₁, v₂, ..., v₂₅₆]  [d₁, d₂, ..., d₂₅₆]                         │
│   ↑ 256个语义Token      ↑ 256个空间Token                              │
│   每个 vᵢ ∈ R^1152     每个 dᵢ ∈ R^1024                              │
│       │               │                                               │
│       └───────┬───────┘                                               │
│               │ 逐 Patch 拼接 (Concatenation)                         │
│               ▼                                                       │
│  [v₁⊕d₁, v₂⊕d₂, ..., v₂₅₆⊕d₂₅₆]                                  │
│   ↑ 256 个融合 Token，每个 ∈ R^2176                                   │
│               │                                                       │
│               ▼                                                       │
│  ┌────────────────────────┐                                           │
│  │ Projection MLP         │                                           │
│  │ R^2176 → R^4096        │                                           │
│  │ (映射到 LLM 维度)      │                                           │
│  └────────────┬───────────┘                                           │
│               │                                                       │
│               ▼                                                       │
│  [p₁, p₂, ..., p₂₅₆]  ← 256 个投影后的视觉 Token                   │
│               │            每个 pᵢ ∈ R^4096 (匹配 Llama 2 维度)       │
│               ▼                                                       │
│  ╔════════════════════════════════════╗                                │
│  ║   Llama 2 7B                      ║                                │
│  ║   [p₁...p₂₅₆] + [text_tokens]    ║                                │
│  ║   → Self-Attention → 输出         ║                                │
│  ╚════════════════════════════════════╝                                │
│                                                                       │
└──────────────────────────────────────────────────────────────────────┘
```

### (b) 融合的数学表达

```
给定输入图像 I ∈ R^(224×224×3):

  Step 1: 两个编码器分别提取 Patch Token
    V = SigLIP(I) ∈ R^(N×d_s)      N=256 patches, d_s=1152
    D = DINOv2(I) ∈ R^(N×d_d)      N=256 patches, d_d=1024

  Step 2: 逐 Patch 拼接
    F_i = [V_i ; D_i] ∈ R^(d_s + d_d)     对每个 patch i = 1,...,N
    F = [F_1, F_2, ..., F_N] ∈ R^(N × 2176)

  Step 3: 投影到 LLM 维度
    P = MLP(F) ∈ R^(N × d_LLM)     d_LLM = 4096 (Llama 2)
    MLP: Linear(2176, 4096) → GELU → Linear(4096, 4096)

  Step 4: 送入 LLM
    输入序列 = [P_1, ..., P_256, text_token_1, ..., text_token_M]
```

### (c) 为什么用拼接而不是其他融合方式？

```
方案 1: 加法融合 (Addition)
  F_i = V_i + D_i
  问题: 要求 d_s = d_d（维度必须相同）
        且会丢失各编码器的独立信息

方案 2: 注意力融合 (Cross-Attention)
  F_i = CrossAttn(Q=V, K=D, V=D)
  问题: 引入额外参数和计算量
        训练不稳定

方案 3: 拼接融合 (Concatenation) ✅ Prismatic 的选择
  F_i = [V_i ; D_i]
  优点: ① 保留了两个编码器的完整信息
        ② 不要求维度相同
        ③ 零额外参数（只需要调整投影层的输入维度）
        ④ 让 LLM 自己学习如何融合两种信息
```

### (d) Prismatic VLM 的实验发现

Prismatic VLM 论文系统对比了多种视觉编码器组合：

| 编码器组合 | VQA 准确率 | 空间定位 | 综合评分 |
| :--- | :--- | :--- | :--- |
| CLIP ViT-L (单编码器) | 79.2 | 62.1 | 70.7 |
| SigLIP ViT-SO (单编码器) | 80.8 | 64.3 | 72.6 |
| DINOv2 ViT-L (单编码器) | 74.1 | **71.8** | 72.9 |
| **SigLIP + DINOv2 (双编码器)** | **81.3** | **70.5** | **75.9** |

**关键结论**：
- DINOv2 单独使用时语义理解较弱（74.1），但空间定位最强（71.8）
- SigLIP 单独使用时语义最强（80.8），但空间定位一般（64.3）
- **双编码器组合取得了两方面的最优或近优**，综合评分大幅领先

---

## 7. 双编码器的训练策略

### (a) 完整训练流程

双编码器在 VLA 中的训练分为**多个阶段**，每个阶段的训练策略不同：

```
╔══════════════════════════════════════════════════════════════════╗
║                    阶段 0: 编码器预训练 (独立)                    ║
║                    (由原作者完成，VLA 直接使用预训练权重)          ║
╠══════════════════════════════════════════════════════════════════╣
║                                                                   ║
║  SigLIP 预训练:                                                   ║
║    数据: 数十亿互联网图文对                                       ║
║    方法: Sigmoid 对比学习                                         ║
║    目标: 图文语义对齐                                             ║
║    结果: SigLIP ViT-SO/14 (400M params)                           ║
║                                                                   ║
║  DINOv2 预训练:                                                   ║
║    数据: LVD-142M (1.42亿张精选图像，无标注)                      ║
║    方法: 自蒸馏 (DINO + iBOT + KoLeo)                             ║
║    目标: 视觉空间表示                                             ║
║    结果: DINOv2 ViT-L/14 (304M params)                            ║
║                                                                   ║
╚══════════════════════════════════════════════════════════════════╝
                              ↓
╔══════════════════════════════════════════════════════════════════╗
║                    阶段 1: VLM 预训练 (Prismatic)                 ║
╠══════════════════════════════════════════════════════════════════╣
║                                                                   ║
║  冻结: SigLIP ✅  DINOv2 ✅                                      ║
║  训练: Projection MLP + LLM (Llama 2)                             ║
║                                                                   ║
║  数据: ~600K 图文对 (LLaVA-style)                                 ║
║  目标: 让 LLM 学会理解双编码器的融合特征                          ║
║  损失: 标准自回归语言建模 (next token prediction)                  ║
║                                                                   ║
║  为什么冻结编码器？                                               ║
║    → 编码器已经通过大规模预训练学到了强大的特征                    ║
║    → 微调可能破坏已学到的表示 (catastrophic forgetting)            ║
║    → 这个阶段只需要学习"翻译"——把视觉特征翻译成 LLM 能懂的语言  ║
║                                                                   ║
╚══════════════════════════════════════════════════════════════════╝
                              ↓
╔══════════════════════════════════════════════════════════════════╗
║                    阶段 2: VLM 指令微调                           ║
╠══════════════════════════════════════════════════════════════════╣
║                                                                   ║
║  解冻: SigLIP ✅  DINOv2 ✅  (可选: 低学习率微调编码器)           ║
║  训练: 全部参数 (端到端)                                          ║
║                                                                   ║
║  数据: ~665K 多任务指令数据 (VQA、Caption、推理等)                ║
║  目标: 让模型学会遵循指令回答视觉问题                             ║
║                                                                   ║
╚══════════════════════════════════════════════════════════════════╝
                              ↓
╔══════════════════════════════════════════════════════════════════╗
║                    阶段 3: VLA 动作微调 (OpenVLA 特有)             ║
╠══════════════════════════════════════════════════════════════════╣
║                                                                   ║
║  训练: 全部参数 (端到端微调)                                      ║
║                                                                   ║
║  数据: 970K 机器人操作轨迹 (Open X-Embodiment)                    ║
║  目标: 让模型输出机器人动作 Token                                  ║
║  损失: Cross-Entropy (动作 Token 预测)                             ║
║                                                                   ║
║  关键: 视觉编码器的空间特征在这里至关重要                          ║
║        → DINOv2 提供的精确空间定位直接帮助动作预测                 ║
║                                                                   ║
╚══════════════════════════════════════════════════════════════════╝
```

### (b) 训练中各组件的学习率策略

```
典型的学习率设置:

组件                    阶段1 (VLM预训练)    阶段2 (指令微调)    阶段3 (VLA微调)
─────────────────────────────────────────────────────────────────────────
SigLIP 编码器           冻结 (lr=0)          2e-6 (极低)        2e-6
DINOv2 编码器           冻结 (lr=0)          2e-6 (极低)        2e-6
Projection MLP          2e-3 (高)            2e-5               2e-5
LLM 骨干 (Llama 2)     2e-5                 2e-5               2e-5

策略: 编码器用极低学习率"微调"，防止破坏预训练特征
      投影层先快速学习"翻译"，再慢慢精调
      LLM 全程中等学习率，持续适应新模态
```

---

## 8. 信息流可视化：从像素到动作

```
完整的信息流 (以 OpenVLA 抓取红色杯子为例):

输入: 📷 桌面图像 (红色杯子在右侧) + 💬 "pick up the red cup"

Step 1: SigLIP 处理
  ┌────────────────────────────────────────┐
  │ SigLIP Patch Tokens:                   │
  │                                        │
  │  [table] [table] [red_obj] [red_obj]   │  ← 每个 Patch 编码语义
  │  [table] [table] [red_obj] [red_obj]   │     "这里有红色物体"
  │  [floor] [floor] [table]   [table]     │     但边界模糊
  │                                        │
  │  特点: 与"red cup"文本高度对齐         │
  └────────────────────────────────────────┘

Step 2: DINOv2 处理
  ┌────────────────────────────────────────┐
  │ DINOv2 Patch Tokens:                   │
  │                                        │
  │  [flat] [flat] [curv] [curv+edge]      │  ← 每个 Patch 编码空间
  │  [flat] [flat] [curv] [curv+edge]      │     精确的纹理和边界
  │  [dark] [dark] [flat]  [flat+edge]     │     "这里有个圆柱体的边缘"
  │                                        │
  │  特点: 物体边界精确，表面几何清晰      │
  └────────────────────────────────────────┘

Step 3: 拼接融合
  ┌────────────────────────────────────────┐
  │ 融合 Token (对应图像右上角 Patch):      │
  │                                        │
  │  [red_obj ⊕ curv+edge]                 │
  │   ↑ "这是红色物体" + "这里是圆柱边缘" │
  │                                        │
  │  → LLM 同时知道: 这是红色杯子 (语义)  │
  │                   杯子边缘在这个精确    │
  │                   位置 (空间)           │
  └────────────────────────────────────────┘

Step 4: LLM + 动作输出
  LLM 结合语义+空间信息:
    "指令要抓红色杯子"
    "杯子在 (x=0.35, y=0.12, z=0.08) 位置"
    "杯子是圆柱形，需要侧面夹取"
    → 输出精确的 7 维动作向量
```

---

## 9. 其他双/多编码器方案

### (a) 各 VLA 模型的视觉编码器选择

| VLA 模型 | 视觉编码器 | 融合方式 | 说明 |
| :--- | :--- | :--- | :--- |
| **RT-2** | CLIP ViT-G (单编码器) | — | 早期工作，只用语义编码器 |
| **OpenVLA** | SigLIP + DINOv2 | Patch 级拼接 | 标杆双编码器方案 |
| **π0** | SigLIP (from PaliGemma) | — | 单编码器但加了 Action Expert |
| **Octo** | 从头训练 ViT | — | 灵活但需要更多数据 |
| **HPT** | 多种编码器 (可插拔) | Stem Tokenizer | 统一不同编码器的输出格式 |
| **SpatialVLA** | SigLIP + Depth Encoder | 多流融合 | 加入深度图编码器 |

### (b) 未来方向

```
趋势 1: 三编码器 (语义 + 空间 + 深度)
  SigLIP + DINOv2 + 深度估计器 → 更完整的 3D 空间理解

趋势 2: 可学习的融合权重
  不同任务可能需要不同比例的语义/空间信息
  → 自适应融合: F = α·V + (1-α)·D, α 由任务决定

趋势 3: 统一编码器
  训练一个同时具备语义和空间能力的编码器
  → 减少参数量和推理时间
  → 但目前的训练方法还做不到两者兼顾

趋势 4: 多分辨率编码
  ViT 使用不同大小的 Patch (14×14, 7×7, 28×28)
  → 同时捕获粗粒度语义和细粒度空间
```

---

## 10. 总结

```
┌─────────────────────────────────────────────────────────────────┐
│                    双视觉编码器总结                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  SigLIP (语义编码器):                                            │
│    训练方式: Sigmoid 对比学习 (图文对)                           │
│    擅长:     "这是什么？" → 语义理解、指令匹配                  │
│    弱项:     "精确在哪？" → 空间定位模糊                        │
│                                                                  │
│  DINOv2 (空间编码器):                                            │
│    训练方式: 自蒸馏 (DINO + iBOT，纯图像)                       │
│    擅长:     "精确在哪？" → 物体边界、像素级定位                │
│    弱项:     "这是什么？" → 无语言对齐                          │
│                                                                  │
│  融合方式 (Prismatic):                                           │
│    方法: 逐 Patch 特征拼接 → 投影 MLP → 送入 LLM               │
│    效果: 语义 + 空间 = 看得懂 + 看得准                          │
│                                                                  │
│  在 VLA 中的价值:                                                │
│    → SigLIP 负责理解指令中的物体是什么                          │
│    → DINOv2 负责精确定位物体在哪里                              │
│    → LLM 融合两者，输出精准的机器人动作                         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 参考文献

1. **Radford, A., Kim, J. W., et al.** (2021). *Learning Transferable Visual Models From Natural Language Supervision (CLIP).* ICML 2021.
2. **Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L.** (2023). *Sigmoid Loss for Language Image Pre-Training (SigLIP).* ICCV 2023.
3. **Oquab, M., Darcet, T., et al.** (2024). *DINOv2: Learning Robust Visual Features without Supervision.* TMLR 2024.
4. **Caron, M., Touvron, H., et al.** (2021). *Emerging Properties in Self-Supervised Vision Transformers (DINO).* ICCV 2021.
5. **Karamcheti, S., et al.** (2024). *Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models.* ICML 2024.
6. **Kim, M. J., Pertsch, K., et al.** (2024). *OpenVLA: An Open-Source Vision-Language-Action Model.* CoRL 2024.
7. **Zhou, J., Wei, C., et al.** (2022). *iBOT: Image BERT Pre-Training with Online Tokenizer.* ICLR 2022.

---

[⬅️ 返回 VLA 目录](./README.md) | [📖 19. VLA 架构](./19_VLA_Architecture.md) | [📖 21. OpenVLA](./21_OpenVLA.md)
