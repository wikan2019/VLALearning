# 18. RT-1 到 RT-2：VLA 的诞生

## 1. 背景：为什么 Google DeepMind 要做 Robotics Transformer？

在 2022 年之前，机器人学习的主流方法是**小模型 + 小数据集 + 单任务**：
*   每个任务（如"抓取红色方块"）训练一个独立的小策略网络。
*   换一个物体（蓝色方块）就要重新训练。
*   完全没有语言理解能力——你不能用自然语言下指令。

Google DeepMind 的核心洞察：**NLP 领域的 Scaling Law（大数据 + 大模型 = 涌现能力）也许在机器人领域同样成立。**

## 2. RT-1：Robotics Transformer（2022.12）

### (a) 核心思想
用 **Transformer** 替代传统的小型策略网络，在**大规模机器人数据**上训练。

### (b) 架构

```
输入：
  [RGB 图像 (300x300)] + [语言指令 "pick up the can"]
      ↓
  图像编码器 (EfficientNet-B3) → 视觉 Token
      ↓
  语言编码器 (FiLM) → 条件注入
      ↓
  Transformer Decoder (8 层) → TokenLearner 压缩
      ↓
输出：
  离散化的动作 Token → 解码为 [Δx, Δy, Δz, Δyaw, gripper, mode]
```

**关键设计**：
*   **动作离散化**：将连续动作空间每个维度切分为 256 个 Bin，转化为分类问题。
*   **TokenLearner**：将视觉 Token 从 81 个压缩到 8 个，大幅降低计算量。
*   **FiLM 条件注入**：语言指令通过 Feature-wise Linear Modulation 注入视觉特征。

### (c) 数据规模

| 指标 | 数值 |
| :--- | :--- |
| 训练数据 | **130,000 条演示轨迹** |
| 任务数量 | **700+ 不同任务** |
| 采集机器人 | 13 台 Everyday Robot |
| 采集时间 | 17 个月 |
| 环境 | Google 办公室厨房 |

### (d) 关键成果
*   在训练任务上成功率 **97%**。
*   对新物体的零样本泛化率 **76%**（之前的方法 < 30%）。
*   首次证明了**大规模机器人数据 + Transformer 架构的威力**。

### (e) 局限
*   **不理解语义**：它只是学会了"指令字符串 → 动作"的映射，并没有真正理解"can"是什么。
*   **泛化有限**：面对训练中从未出现过的指令（如"pick up something heavy"），完全无法处理。
*   **知识孤岛**：模型只在机器人数据上训练，没有利用互联网上的海量视觉-语言知识。

---

## 3. RT-2：Vision-Language-Action Model（2023.07）

### (a) 核心突破

RT-2 的核心洞察震撼了整个机器人学界：

> **"不需要从头训练机器人模型——直接拿一个已有的 VLM，把机器人动作当成'另一种语言'来输出就行了。"**

具体做法：
1.  取一个在互联网数据上预训练好的 VLM（PaLI-X 55B 或 PaLM-E 12B）。
2.  把机器人动作**编码为文本字符串**：`[0.02, -0.01, 0.05, 0, 0.1, 0, 1]` → `"2 -1 5 0 10 0 1"`。
3.  在机器人数据上 **co-fine-tune**，让模型同时保持 VQA 能力和学习机器人控制。

### (b) 架构

```
输入：
  [RGB 图像] + "pick up the bottle that is closest to the edge of the table"
      ↓
  预训练 VLM (PaLI-X 55B / PaLM-E 12B)
      ↓
  自回归输出 Token 序列
      ↓
输出（两种模式）：
  模式 1 (VQA): "The bottle is a blue plastic water bottle..."
  模式 2 (Action): "1 128 91 241 5 101 128"  ← 这就是动作！
```

**关键创新：动作即语言 (Action as Language)**
*   每个动作维度被量化为 [0, 255] 的整数。
*   7 维动作变成 7 个整数 Token，直接接在语言 Token 后面自回归生成。
*   模型无需任何架构修改——动作只是"另一种语言"。

### (b-2) Co-Fine-Tuning 详解 ⭐

**Co-fine-tuning（共同微调）** 是 RT-2 最关键的训练策略，也是它能成功的核心秘密。

#### 什么是 Co-Fine-Tuning？

Co-fine-tuning = **co-training（共同训练）+ fine-tuning（微调）** 的结合。

它的含义是：在微调 VLM 学习机器人动作的同时，**继续混入原始的互联网视觉-语言数据一起训练**，而不是只用机器人数据微调。

```
❌ 普通 Fine-Tuning（只用机器人数据）：
  预训练 VLM ──→ [只喂机器人数据] ──→ 学会了动作，但忘记了VQA能力
                                        ↑ 灾难性遗忘！

✅ Co-Fine-Tuning（混合数据）：
  预训练 VLM ──→ [机器人数据 + 原始 VL 数据 混合喂入] ──→ 既会动作，又保持VQA
                                                           ↑ 知识保留！
```

#### 为什么需要 Co-Fine-Tuning？

直接用机器人数据微调 VLM 会导致**灾难性遗忘 (Catastrophic Forgetting)**：

| 场景 | 普通 Fine-Tuning | Co-Fine-Tuning |
| :--- | :--- | :--- |
| 微调前："这是什么？" → "这是一个红色的杯子" | ✅ 正常回答 | ✅ 正常回答 |
| 微调后：机器人控制 | ✅ 学会了 | ✅ 学会了 |
| 微调后："这是什么？" → "1 128 91 241 5 101 128" | ❌ **VQA 能力丧失！** | ✅ **仍能正常回答** |

如果 VLM 忘掉了"杯子是什么"、"国旗代表什么国家"等互联网知识，那 RT-2 引以为傲的涌现能力（语义推理、类比推理等）就会全部消失——模型退化为一个"只会输出动作的傀儡"。

#### Co-Fine-Tuning 的具体做法

```
每个训练 Batch 包含两类数据：

┌──────────────────────────────────────────┐
│              训练 Batch                    │
│                                          │
│  📦 机器人数据（上采样/加权）              │
│     输入: [图像] + "pick up the can"      │
│     标签: "1 128 91 241 5 101 128"        │
│                                          │
│  📦 原始 VL 数据（保留部分）               │
│     输入: [图像] + "what is in this image?"│
│     标签: "A cat sitting on a couch"      │
│                                          │
│  📦 原始 VL 数据                          │
│     输入: [图像] + "describe this scene"   │
│     标签: "A busy street in Tokyo..."     │
│                                          │
└──────────────────────────────────────────┘
                    ↓
      同一个模型，同一个损失函数，同时优化
```

**关键细节**：
1.  **数据混合**：每个 Batch 同时包含机器人轨迹数据和原始 VQA/Caption 数据。
2.  **机器人数据上采样（Up-Weighting）**：因为机器人数据量远小于互联网数据，需要增加机器人样本的采样权重，确保模型充分学习动作输出（详见下方"采样权重详解"）。
3.  **统一格式**：所有数据格式统一——输入都是"图像+文本"，输出都是"文本 Token"。机器人动作只是一种特殊的"文本输出"。
4.  **解码约束**：推理时，如果任务是机器人控制，则约束采样只输出合法的动作 Token（0-255 的整数）。

#### 采样权重详解：Up-Weighting 是怎么实现的？⭐

RT-2 原文（Section 3.2）原话：

> *"During co-fine-tuning we balance the ratios of robot and web data in each training batch by increasing the sampling weight on the robot dataset."*

**核心问题**：机器人数据和互联网数据的规模差距极其悬殊：

```
互联网 VL 数据（PaLI-X / PaLM-E 预训练数据）：数十亿条样本
机器人轨迹数据（RT-1 采集）：                   ~130,000 条轨迹

自然比例：机器人数据占比 < 0.001%
→ 如果按自然比例采样，模型几乎"看不到"机器人数据
```

**解决方案：增加机器人数据的采样概率（Sampling Weight / Sampling Probability）**。

##### 原理：加权随机采样

在多数据源训练中，每个训练 Batch 的样本是通过**加权随机采样**从不同数据源中抽取的。每个数据源有一个**采样概率（Sampling Probability）**，决定了该数据源在每个 Batch 中被选中的频率。

```
按自然比例采样（❌ 不可行）：
  ┌───────────────────────────────────────────┐
  │ 一个 Batch (1024 个样本)                    │
  │ ████████████████████████████████░          │
  │ ↑ 99.99% 互联网数据   ↑ 0.01% 机器人数据   │
  │                        (约 0.1 条!)         │
  └───────────────────────────────────────────┘
  → 模型几乎学不到机器人动作

加权采样（✅ RT-2 的做法）：
  ┌───────────────────────────────────────────┐
  │ 一个 Batch (1024 个样本)                    │
  │ ████████████████████░░░░░░░░░░░░░░░░░░░░  │
  │ ↑ ~50% 互联网数据     ↑ ~50% 机器人数据    │
  └───────────────────────────────────────────┘
  → 模型在每个 Batch 都能充分学习两类数据
```

##### 具体实现方式

深度学习中实现 Up-Weighting 主要有 **3 种等价方式**：

**方式 1：直接设置采样概率 (Sampling Probability)**

```python
# 伪代码
datasets = {
    "web_vqa":    WebVQADataset,      # 数十亿条
    "robot_data": RobotDataset,       # ~130K 条
}

# ❌ 自然比例：按数据量比例采样
# natural_probs = {"web_vqa": 0.9999, "robot_data": 0.0001}

# ✅ Up-Weighting：人为提高机器人数据的采样概率
sampling_probs = {
    "web_vqa":    0.5,   # 50% 的样本来自互联网
    "robot_data": 0.5,   # 50% 的样本来自机器人  ← 放大了 ~5000 倍！
}

# 每个 Batch：先掷骰子决定从哪个数据源采样，再从该数据源随机取一条
for step in training_loop:
    batch = []
    for _ in range(batch_size):
        source = random.choice(["web_vqa", "robot_data"], p=[0.5, 0.5])
        sample = datasets[source].random_sample()
        batch.append(sample)
    loss = model.forward(batch)
    loss.backward()
```

**方式 2：数据重复（Oversampling / Repetition）**

```
原始数据池：
  互联网数据: [W₁, W₂, W₃, ..., W₁₀₀₀₀₀₀₀₀₀]  (10 亿条)
  机器人数据: [R₁, R₂, R₃, ..., R₁₃₀₀₀₀]         (13 万条)

重复后的数据池（机器人数据被重复 ~7700 次）：
  互联网数据: [W₁, W₂, ..., W₁₀₀₀₀₀₀₀₀₀]         (10 亿条)
  机器人数据: [R₁, R₂, ..., R₁₃₀₀₀₀] × 7700       (~10 亿条)

→ 两个数据源大小接近，均匀采样即可
```

**方式 3：温度采样（Temperature-Based Mixing）**

这是 Google 在多任务 VLM 训练中常用的方法：

```
给定 K 个数据源，每个数据源有 nᵢ 条数据。
采样概率公式：

                 nᵢ^(1/T)
    pᵢ = ─────────────────────
          Σⱼ nⱼ^(1/T)

T = 温度参数：
  T = 1   → 按自然比例（大数据源主导）
  T → ∞   → 完全均匀（所有数据源等概率）
  T = 中间值 → 折中方案
```

```
举例（假设互联网 1B 条，机器人 130K 条）：

T = 1 (自然比例):   p_web = 99.99%,  p_robot = 0.01%  ← 不可行
T = 2:               p_web = 98.9%,   p_robot = 1.1%   ← 仍然太少
T = 5:               p_web = 83.2%,   p_robot = 16.8%  ← 可以考虑
T = ∞ (均匀):        p_web = 50%,     p_robot = 50%    ← 完全均等
```

RT-2 论文中使用的是 PaLI-X / PaLM-E 原论文的训练超参数，这类 Google VLM 通常采用温度采样或直接设定采样比例来混合数据。

##### 为什么不能把机器人数据的比例设得太高？

Up-Weighting 需要找到一个**平衡点**——机器人比例太低或太高都有问题：

```
机器人占比过低 (如 1%)：
  → 模型学不会动作输出
  → 在机器人控制任务上表现差

机器人占比过高 (如 99%)：
  → 退化为普通 Fine-Tuning
  → 互联网知识被遗忘（灾难性遗忘）
  → 涌现能力消失

最优区间 (论文实验得出)：
  → 平衡点：让模型在每个 Batch 中同时看到足够的两类数据
  → 论文表述为 "balance the ratios"（平衡比例）
```

##### 机器人数据会被重复看到很多次，会过拟合吗？

是的，这是一个实际风险。机器人数据只有 ~130K 条，在整个训练过程中每条数据会被模型看到**数千到上万次**。缓解措施包括：

| 措施 | 说明 |
| :--- | :--- |
| **数据增强** | 对机器人图像做随机裁剪、色彩抖动等 |
| **正则化** | Dropout、Weight Decay 等标准正则化技术 |
| **互联网数据的正则效果** | 混合的互联网数据本身就是一种隐式正则——它迫使模型保持通用特征表示，不会过度拟合到机器人数据 |
| **早停 (Early Stopping)** | 监控验证集性能，在过拟合前停止训练 |

##### 完整流程图

```
┌─────────────────────────────────────────────────────────────┐
│                Co-Fine-Tuning 的数据流                        │
│                                                             │
│  数据源 A: 互联网 VL 数据 (数十亿条)                          │
│    ├─ VQA: "Q: What color is the car? A: Red"               │
│    ├─ Caption: "A dog running in the park"                   │
│    └─ 其他 VL 任务                                           │
│                                                             │
│  数据源 B: 机器人轨迹数据 (~130K 条)                           │
│    └─ "Q: What action should the robot take to              │
│        pick up the can? A: 1 128 91 241 5 101 128"          │
│                                                             │
│         ↓ 加权随机采样 (Up-Weighting 机器人数据)               │
│                                                             │
│  ┌─────────────────────────────┐                             │
│  │ 训练 Batch (混合)            │                             │
│  │  sample 1: 机器人 (动作)     │                             │
│  │  sample 2: VQA (文本)        │                             │
│  │  sample 3: 机器人 (动作)     │                             │
│  │  sample 4: Caption (文本)    │                             │
│  │  sample 5: 机器人 (动作)     │                             │
│  │  ...                        │                             │
│  └─────────────────────────────┘                             │
│                    ↓                                         │
│  统一的自回归损失函数（CrossEntropyLoss）                       │
│  → 不管输出是"Red"还是"1 128 91 241 5 101 128"              │
│  → 都是 next-token prediction                                │
│                    ↓                                         │
│  模型参数更新（梯度来自两类数据的混合）                         │
└─────────────────────────────────────────────────────────────┘
```

#### 类比理解

```
Co-Fine-Tuning 就像：
  一个精通英语的翻译家 🧑‍🏫，要学习日语
  
  ❌ 普通 Fine-Tuning：
     只练日语 → 3 个月后日语流利了，但英语忘了 😱

  ✅ Co-Fine-Tuning：
     每天 60% 时间学日语 + 40% 时间继续用英语工作
     → 3 个月后：日语流利 + 英语也没忘 ✅
```

#### Co-Fine-Tuning 的效果

| 指标 | 只用机器人数据微调 | Co-Fine-Tuning |
| :--- | :--- | :--- |
| 机器人控制（已见任务） | ✅ 正常 | ✅ 正常 |
| 机器人控制（新物体） | 弱 | **强（保留了物体知识）** |
| 语义推理（"最小的"） | ❌ 丧失 | **✅ 保留** |
| 符号理解（国旗） | ❌ 丧失 | **✅ 保留** |
| 类比推理（"当锤子用"） | ❌ 丧失 | **✅ 保留** |
| VQA 问答能力 | ❌ 丧失 | **✅ 保留** |

> **一句话总结 Co-Fine-Tuning**：在教 VLM 说"机器人语言"（动作 Token）的同时，不让它忘记"人类语言"（VQA）——通过混合训练数据，让模型既能控制机器人，又保留了互联网知识带来的推理和理解能力。这正是 RT-2 涌现能力的根源。

### (c) 对比 RT-1 vs RT-2

| 特性 | RT-1 | RT-2 |
| :--- | :--- | :--- |
| **底层模型** | 从零训练的 Transformer | **预训练 VLM (PaLI-X / PaLM-E)** |
| **参数量** | ~35M | **12B - 55B** |
| **训练数据** | 仅机器人数据 (130K) | **互联网 VL 数据 + 机器人数据** |
| **语义理解** | ❌ 字符串匹配 | ✅ 真正理解语义 |
| **新指令泛化** | ❌ | ✅ 零样本理解从未见过的指令 |
| **动作输出** | 分类头 (每维 256 Bin) | **文本 Token 自回归** |
| **涌现能力** | ❌ | ✅ 推理、类比、链式思考 |

### (d) RT-2 的涌现能力（最惊艳的部分）

RT-2 展现了 RT-1 完全不具备的**涌现能力 (Emergent Capabilities)**：

| 涌现能力 | 举例 | 为什么 RT-1 做不到 |
| :--- | :--- | :--- |
| **语义推理** | "把最小的物体放到碗里" — 模型能比较大小 | RT-1 不知道"最小"是什么意思 |
| **符号理解** | "把一个国家的国旗移到另一个" — 模型认识国旗 | RT-1 没有国旗的知识 |
| **类比推理** | "把可以当锤子的东西递给我" — 模型挑了石头 | RT-1 不理解"功能类比" |
| **Chain-of-Thought** | 内部推理链："这个场景里...石头最硬最重...适合当锤子" | RT-1 没有推理能力 |

这些能力**不是在机器人数据中学到的**——它们来自 VLM 在互联网上学到的世界知识，通过 co-fine-tuning（详见上方 b-2 节）被"激活"并连接到了机器人的动作空间。正因为 co-fine-tuning 保留了 VLM 的原始知识，这些涌现能力才不会在微调中被遗忘。

### (e) RT-2-X：跨具身扩展

RT-2-X 是 RT-2 在 **Open X-Embodiment** 数据集（多个机器人平台）上训练的版本：
*   在原始 RT-2 的数据基础上，加入了来自其他实验室的多机器人数据。
*   在不同机器人上的泛化性能提升了 **3 倍**。
*   成为后续 OpenVLA 等工作的对标基线。

---

## 4. 为什么 RT-2 是里程碑？

RT-2 的意义不在于某个具体指标提升了多少，而在于它**证明了一个范式**：

```
互联网知识（VLM）→ 物理动作（VLA）
```

具体来说：
1.  **VLM 的知识是可迁移的**：在互联网上学到的"杯子长什么样"直接帮助机器人抓杯子。
2.  **动作可以被编码为 Token**：不需要为机器人设计专门的架构，通用的自回归框架就够了。
3.  **Scaling Law 适用于机器人**：更大的 VLM 底座 → 更强的机器人控制能力。

> **一句话总结**：RT-1 证明了"大数据 + Transformer"对机器人有效；RT-2 则证明了"VLM 的知识可以直接驱动机器人"——这一洞察开创了整个 VLA 领域。
