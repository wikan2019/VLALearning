# WM-07. 机器人世界模型与规划

## 1. 为什么机器人特别需要世界模型？

当前主流 VLA（如 OpenVLA, π0）是**反应式**的——看到什么就做什么，不做规划。

```
反应式 VLA:   [看到杯子] → 直接伸手抓 → 啊，前面有障碍物！→ 碰撞 ❌
规划式 VLA:   [看到杯子] → 🧠 想象：直线会撞到障碍物
                          → 🧠 想象：绕右边可以避开
                          → 执行绕右方案 ✅
```

世界模型让机器人拥有**"三思而后行"**的能力。

## 2. 机器人世界模型的两种路线

### 路线一：潜在空间世界模型（轻量）

```
[当前状态] →(动作)→ [预测的潜在状态] → 评估好坏
                         ↑ 不生成视频，只预测抽象表示
```

*   **代表**：Dreamer V3, TD-MPC2, V-JEPA 2
*   **优势**：快速、可以实时规划
*   **劣势**：不直观，难以可视化和调试

### 路线二：视频世界模型（重量）

```
[当前帧] →(动作)→ [预测的未来视频帧序列] → 评估目标是否达成
                         ↑ 生成逼真的视频预测
```

*   **代表**：UniPi, iVideoGPT, ViPRA, World-VLA-Loop
*   **优势**：直观、可视化、与 VLM 兼容
*   **劣势**：计算量大、预测可能模糊

## 3. 代表方法详解

### (a) UniPi：视频预测 → 逆动力学 → 动作

```
步骤 1: [当前帧] + "把杯子放到盘子上" → 视频扩散模型 → [预测的操作视频]
步骤 2: [预测视频] → 逆动力学模型 → [每帧对应的动作序列]
步骤 3: 执行动作序列
```

**核心思想**：先规划"世界应该怎么变化"（视频），再反推"我应该怎么做"（动作）。

**优势**：
*   视频数据（YouTube）远比机器人数据丰富，视频模型泛化能力更强。
*   "目标视频"比"目标动作"更直观、更容易指定。

### (b) iVideoGPT：统一 Token 世界模型

将观测、动作、奖励统一为 Token 序列，用自回归 Transformer 建模整个交互过程：

```
[Obs₁] [Act₁] [Rew₁] [Obs₂] [Act₂] [Rew₂] ...
                ↓
    自回归 Transformer (Next Token Prediction)
                ↓
    给定历史，预测未来的 Obs/Rew → 评估不同动作的好坏
```

*   在百万级操作轨迹上预训练。
*   支持视觉规划和 Model-Based RL。

### (c) ViPRA：从无动作视频学习

利用光流自动提取"隐式动作"，从纯观察视频中学习世界模型：

```
[YouTube 操作视频] → 光流分析 → 隐式动作 → 训练世界模型 + 策略
                    ↑ 不需要任何动作标签！
```

**意义**：解锁了互联网上海量无标注操作视频作为训练数据。

### (d) World-VLA-Loop：闭环学习

世界模型和 VLA 策略**互相提升**：

```
循环 1: VLA 在世界模型中执行 → 收集经验 → 世界模型学习失败模式
循环 2: 更准确的世界模型 → VLA 在其中练习 → 策略变强
循环 3: 更强的 VLA → 探索更多场景 → 世界模型更全面
...
```

## 4. 世界模型在机器人规划中的应用

### (a) 目标条件规划

```
输入: [当前图像] + [目标图像（桌子整理好的样子）]
世界模型: 在潜在空间搜索从"当前"到"目标"的最短路径
输出: [最优动作序列]
```

### (b) 安全约束规划

```
世界模型预测: "如果推力太大，杯子会被推到桌子边缘"
安全层: "限制推力在安全范围内"
→ 减少真实环境中的失败和损坏
```

### (c) 长程任务分解

```
任务: "做一份三明治"
世界模型:
  子目标 1: 拿面包     → 预测执行后的世界状态 ✓
  子目标 2: 抹黄油     → 预测执行后的世界状态 ✓
  子目标 3: 加生菜     → 预测执行后的世界状态 ✓
  子目标 4: 合上面包   → 预测执行后的世界状态 ✓
→ 分解为可执行的子任务序列
```

## 5. V-JEPA 2 的零样本机器人规划

V-JEPA 2 展示了最令人兴奋的能力——**无需任何机器人数据**即可规划：

```
训练: 只在互联网视频上自监督训练（没有机器人数据）
测试: 直接用于机器人规划

过程:
  1. 观察当前场景 → V-JEPA 2 编码为潜在状态
  2. 枚举候选动作序列
  3. 对每个序列，用 V-JEPA 2 预测未来潜在状态
  4. 评估哪个序列的未来状态最接近目标
  5. 执行最优序列
```

## 6. 各方法对比

| 方法 | 世界模型类型 | 需要机器人数据 | 实时性 | 规划能力 |
| :--- | :--- | :--- | :--- | :--- |
| UniPi | 视频扩散 | 逆动力学需要 | 慢 | 强 |
| iVideoGPT | 自回归 Token | ✅ | 中等 | 强 |
| ViPRA | 视频预测+光流 | ❌ | 中等 | 中 |
| V-JEPA 2 | 潜在空间预测 | **❌** | **快** | 强 |
| Dreamer V3 | 潜在+解码 | ✅ (RL 交互) | 快 | 强 |
| TD-MPC2 | 潜在隐式 | ✅ (RL 交互) | **最快** | 强 |
| World-VLA-Loop | 视频+RL | 自动生成 | 中等 | **最强(闭环)** |

## 7. 世界模型 vs 反应式 VLA

| 特性 | 反应式 VLA (OpenVLA, π0) | 世界模型 + 规划 |
| :--- | :--- | :--- |
| **决策方式** | 看到什么做什么 | 先想后做 |
| **长程任务** | 弱（短视） | **强（可预演多步）** |
| **安全性** | 试错后才知道 | **脑中试错，更安全** |
| **数据效率** | 需要大量演示 | 可在想象中练习 |
| **推理速度** | 快 | 较慢（需要模拟） |
| **成熟度** | 高 | 中（快速发展中） |

## 8. 发展趋势

| 趋势 | 说明 |
| :--- | :--- |
| **VLA + 世界模型融合** | World-VLA-Loop 预示了两者的深度整合 |
| **零样本规划** | V-JEPA 2 证明了无需机器人数据即可规划 |
| **实时化** | TD-MPC2 级别的速度让世界模型规划可以实时运行 |
| **多模态世界模型** | 同时预测视觉、力觉、声音等多模态状态 |

> **一句话总结**：机器人世界模型让 VLA 从"反应式"升级为"规划式"——在脑中预演多条路径，选最优的执行。V-JEPA 2 的零样本规划和 World-VLA-Loop 的闭环学习代表了两个最前沿方向，预示着未来的机器人将具备真正的"三思而后行"能力。
