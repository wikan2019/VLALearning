# WM-02. Dreamer 系列：在想象中学习的 RL 智能体

## 1. 背景：Model-Free vs Model-Based RL

| 特性 | Model-Free RL | Model-Based RL (World Model) |
| :--- | :--- | :--- |
| **学什么** | 直接学"状态→动作"的映射 | 先学环境模型，再在模型中规划 |
| **数据效率** | 极低（需要大量试错） | **高（在想象中练习）** |
| **类比** | 只靠反复练习学骑自行车 | 先在脑中想象平衡原理，再练习 |
| **代表** | PPO, SAC, DQN | **Dreamer, TD-MPC, MuZero** |

> **核心问题**：Model-Free RL 需要在真实环境中试错上百万次才能学会一个任务。但真实机器人不可能碰撞上百万次——太贵、太慢、太危险。世界模型解决这个问题：**在虚拟的想象中练习**。

## 2. Dreamer 的核心思想

```
┌─────────────────────────────────────────────────────────┐
│                    Dreamer 流程                          │
│                                                         │
│  1. 在真实环境中收集少量经验                              │
│     观测 o₁ → 动作 a₁ → 奖励 r₁ → 观测 o₂ → ...        │
│                        ↓                                │
│  2. 用经验训练世界模型                                   │
│     世界模型学习：f(sₜ, aₜ) → sₜ₊₁, rₜ₊₁              │
│                        ↓                                │
│  3. 在世界模型中"想象"大量轨迹 ← 🔑 关键步骤            │
│     s₁ →(a₁)→ ŝ₂ →(a₂)→ ŝ₃ →(a₃)→ ŝ₄ → ...          │
│     ↑ 不需要真实环境！完全在模型内部                      │
│                        ↓                                │
│  4. 用想象的轨迹优化策略                                 │
│     选择累积奖励最高的动作序列                            │
│                        ↓                                │
│  5. 回到真实环境执行 → 收集更多经验 → 循环                │
└─────────────────────────────────────────────────────────┘
```

## 3. Dreamer V1（2020）

### 架构：RSSM（循环状态空间模型）

```
┌──────────────────────────────────────────────┐
│              RSSM 世界模型                     │
│                                              │
│  观测 oₜ → 编码器 → 后验状态 sₜ              │
│                         ↓                    │
│  sₜ + aₜ → 转换模型 → 先验 sₜ₊₁ (预测)      │
│                         ↓                    │
│  sₜ₊₁ → 解码器 → 预测观测 ôₜ₊₁              │
│  sₜ₊₁ → 奖励头 → 预测奖励 r̂ₜ₊₁              │
└──────────────────────────────────────────────┘
```

**RSSM (Recurrent State-Space Model)** 的关键设计：
*   **确定性路径** (RNN)：捕捉长期记忆。
*   **随机性路径** (VAE)：表示环境中的不确定性。
*   两者结合 → 状态 = (确定性隐状态 h, 随机隐变量 z)。

### 训练目标
*   **重建损失**：预测的观测 ≈ 真实观测。
*   **奖励预测**：预测奖励 ≈ 真实奖励。
*   **KL 正则**：后验分布 ≈ 先验分布（世界模型的预测要自洽）。

## 4. Dreamer V2（2021）

**关键改进**：将潜在空间从**连续** (Gaussian) 改为**离散** (Categorical)。

```
V1: z ~ N(μ, σ²)     ← 连续高斯分布
V2: z ~ Cat(p₁...p₃₂)  ← 32 个类别的离散分布（32 个离散变量，每个 32 类）
```

**为什么离散更好？**
1.  **表达力更强**：32×32=1024 种组合，比连续空间更结构化。
2.  **训练更稳定**：避免了 VAE 的 "posterior collapse" 问题。
3.  **更适合规划**：离散状态空间更容易搜索。

**成果**：在 Atari 游戏上超越了 Model-Free 方法，使用仅 **1/50** 的环境交互数据。

## 5. Dreamer V3（2023，Nature）

### (a) 核心突破

Dreamer V3 发表于 Nature，是世界模型领域的标志性工作：

> **单一配置、零调参，在 150+ 不同类型的任务上超越专用方法。**

包括：
*   Atari 游戏（2D 像素控制）
*   DMControl（连续物理控制）
*   Minecraft（开放世界 3D 生存游戏）
*   Crafter（2D 生存游戏）

### (b) 关键技术

| 技术 | 说明 |
| :--- | :--- |
| **Symlog Predictions** | 对奖励和值函数取 symlog 变换，统一不同任务的数值范围 |
| **Free Bits** | KL 正则的下界，防止世界模型过拟合 |
| **归一化回报** | 自适应归一化奖励，跨任务统一 |
| **大规模离散潜在空间** | 32 个变量 × 32 类 = 1024 种状态 |

### (c) Minecraft 钻石

Dreamer V3 **首次从零开始**（无人类数据、无预定义课程）在 Minecraft 中收集了钻石——这是一个需要 20+ 分钟连续决策、涉及采集→合成→挖矿等多步骤的长程任务。

```
采集木头 → 制作木镐 → 采集石头 → 制作石镐 → 找到铁矿 → 冶炼铁 →
制作铁镐 → 深入地下 → 找到钻石矿 → 采集钻石 ← 全程自主完成
```

## 6. TD-MPC2（2024，ICLR）

### (a) 与 Dreamer 的区别

| 特性 | Dreamer V3 | TD-MPC2 |
| :--- | :--- | :--- |
| **世界模型类型** | 显式（有编码器+解码器） | **隐式（无解码器）** |
| **状态空间** | 离散潜在变量 | 连续潜在向量 |
| **规划方式** | Actor-Critic（策略梯度） | **MPC（模型预测控制）** |
| **动作选择** | 策略网络直接输出 | 在线轨迹优化（MPPI） |

### (b) 隐式世界模型

TD-MPC2 不重建像素——它的世界模型只在**潜在空间**中工作：

```
传统:  sₜ →(aₜ)→ sₜ₊₁ → 解码器 → 像素预测 (计算量大)
TD-MPC2: sₜ →(aₜ)→ sₜ₊₁ → 直接在潜在空间评估价值 (高效)
```

### (c) 关键成果
*   **104 个连续控制任务**，跨越 DMControl、Meta-World、ManiSkill2、MyoSuite。
*   扩展到 **317M 参数**的单一智能体，同时处理 80 个任务。
*   单一超参数配置，无需任务特定调参。

## 7. 各版本对比

| 特性 | Dreamer V1 | Dreamer V2 | Dreamer V3 | TD-MPC2 |
| :--- | :--- | :--- | :--- | :--- |
| **年份** | 2020 | 2021 | 2023 | 2024 |
| **潜在空间** | 连续 (Gaussian) | 离散 (Categorical) | 离散 (32×32) | 连续 |
| **解码器** | 有（像素重建） | 有 | 有 | **无（隐式）** |
| **规划** | Actor-Critic | Actor-Critic | Actor-Critic | **MPC (MPPI)** |
| **任务范围** | 2D 控制 | Atari + 控制 | **150+ 全类型** | 104 连续控制 |
| **里程碑** | 首个 Dreamer | 离散潜在空间 | **Minecraft 钻石** | 317M 跨域 |

## 8. 世界模型 RL 的核心优势

```
真实环境交互：         |████░░░░░░|  10%   ← 只需少量真实数据
想象中的练习（世界模型）：|██████████|  90%   ← 大量虚拟练习

Model-Free RL：        |██████████|  100%  ← 全部需要真实交互
```

> **一句话总结**：Dreamer 系列证明了"在想象中学习"的威力——通过学习环境的内部模型，AI 智能体可以在虚拟想象中练习数千次，只需极少的真实交互就能学会复杂任务。Dreamer V3 以单一配置统治 150+ 任务，TD-MPC2 将世界模型扩展到了百万参数级别的跨域智能体。
