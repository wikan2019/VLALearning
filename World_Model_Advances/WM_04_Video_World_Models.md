# WM-04. 视频世界模型：Sora 的启示

## 1. 核心命题：视频生成 = 世界模拟

OpenAI 在 2024 年 2 月发布 Sora 时，给出了一个影响深远的判断：

> **"Video generation models as world simulators."**（视频生成模型就是世界模拟器。）

这意味着：如果一个模型能生成物理上一致的视频，那它就**隐式地学会了世界的运作规律**。

```
传统物理引擎:  牛顿力学方程 → 计算物体轨迹 → 渲染画面
视频世界模型:  [当前帧 + 条件] → 神经网络 → 预测未来帧序列
              ↑ 不写任何物理方程！模型从数据中自己学会了物理
```

## 2. Sora（2024.02，OpenAI）

### (a) 架构：Diffusion Transformer (DiT)

```
[文本描述] → T5 编码器 → 条件向量
                              ↓
[噪声潜在序列] → DiT (Diffusion Transformer) → 去噪后的潜在序列 → 解码器 → 视频
                   ↑ 时空 Patch 化
```

**关键设计**：
*   **Spacetime Patches**：将视频分割为时空小块（类似 ViT 对图像的处理），统一处理不同分辨率和时长。
*   **Transformer 骨干**：而非传统的 U-Net，便于扩展（scaling）。
*   **可变时长/分辨率**：支持生成不同长度和分辨率的视频。

### (b) 涌现的物理理解

OpenAI 报告 Sora 展现了**初步的物理世界理解**：
*   **3D 一致性**：摄像头移动时，物体保持正确的 3D 关系。
*   **长期一致性**：物体不会突然消失或变形（在数秒内）。
*   **物体持久性**：被遮挡的物体再次出现时保持一致。
*   **简单交互**：模拟画笔在画布上留下痕迹。

### (c) 局限

Sora 仍然不是完美的物理模拟器：
*   **违反物理定律**：物体穿模、悬浮、凭空出现。
*   **因果错误**：打碎玻璃后玻璃自动复原。
*   **长期漂移**：超过 30 秒后，一致性显著下降。
*   **不可交互**：生成后无法实时修改动作输入。

## 3. iVideoGPT（2024.05）

### (a) 核心思想：统一 Token 建模

将视觉观测、动作、奖励全部编码为 Token，用自回归 Transformer 统一建模：

```
Token 序列: [Obs₁ Token] [Act₁ Token] [Rew₁ Token] [Obs₂ Token] [Act₂ Token] ...
                                         ↓
                     标准自回归 Transformer → Next Token Prediction
```

### (b) 关键创新
*   **压缩 Tokenization**：将高维视觉观测压缩为紧凑的离散 Token。
*   **百万级预训练**：在数百万条人类和机器人操作轨迹上预训练。
*   **多用途**：支持条件视频预测、视觉规划、Model-Based RL。

### (c) 与 Sora 的区别

| 特性 | Sora | iVideoGPT |
| :--- | :--- | :--- |
| **架构** | 扩散 Transformer | **自回归 Transformer** |
| **可交互** | ❌ 生成后不可控 | **✅ 动作条件生成** |
| **应用** | 视频创作 | **机器人 + RL** |
| **动作输入** | 无 | **✅ 动作 Token 驱动** |

## 4. Vid2World（2025）

### (a) 核心思想

将**预训练的视频扩散模型**改造为**交互式世界模型**：

```
预训练的视频扩散模型（如 Stable Video Diffusion）
    ↓ "因果化改造" (Video Diffusion Causalization)
交互式世界模型
    ↓ 加入动作条件引导
动作可控的世界模拟器
```

### (b) 关键创新

**视频扩散因果化**：原始的视频扩散模型是"非因果"的（同时看到所有帧），需要改造为"因果"的（只能看到过去的帧）以支持自回归式的交互。

**应用验证**：在机器人操作、3D 游戏、导航等场景中均表现有效。

## 5. WorldDreamer（2024）

### (a) 核心思想

将世界建模框架化为**无监督的视觉序列建模**——类似 BERT 的思路：

```
视频帧序列 → 离散 Token → 遮蔽部分 Token → 预测被遮蔽的 Token
```

### (b) 多功能

支持文本到视频、图像到视频、视频编辑等多种任务，在不同场景中都能作为世界模型使用。

## 6. ViPRA（2025）

### (a) 核心突破：从无动作视频学习

ViPRA 的创新在于**不需要动作标注**——从纯观察视频中学习控制：

```
传统: [视频帧] + [动作标签] → 学习动作条件的世界模型
ViPRA: [视频帧] (无动作标签) → 光流 → "隐式动作" → 世界模型
                                ↑
                    通过光流一致性自动提取动作
```

### (b) 意义

互联网上有海量的操作视频（YouTube 教程等），但没有动作标签。ViPRA 让这些视频都可以用于训练世界模型和机器人策略。

**成果**：SIMPLER 基准 +16%，真实机器人操作 +13%。

## 7. 视频世界模型的核心挑战

| 挑战 | 说明 | 现状 |
| :--- | :--- | :--- |
| **物理一致性** | 生成的视频是否遵守物理定律 | Sora 初步可以，但仍频繁违反 |
| **长时一致性** | 长视频中物体是否保持一致 | 超过 30 秒后显著下降 |
| **可交互性** | 能否实时响应动作输入 | iVideoGPT、Vid2World 已实现 |
| **计算成本** | 视频生成极其昂贵 | 潜在空间压缩可降低 10-100x |
| **评测标准** | 如何评估"物理理解"而非"画质" | WorldModelBench 首次系统化 |

## 8. 各方法对比

| 模型 | 架构 | 可交互 | 需要动作标签 | 开源 | 应用 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Sora** | Diffusion Transformer | ❌ | ❌ | ❌ | 视频创作 |
| **iVideoGPT** | 自回归 Transformer | ✅ | ✅ | ✅ | 机器人 + RL |
| **Vid2World** | 改造后的扩散模型 | ✅ | ✅ | ✅ | 机器人 + 游戏 |
| **WorldDreamer** | 遮蔽预测 | 有限 | ❌ | ✅ | 多场景 |
| **ViPRA** | 视频预测 + 光流 | ✅ | **❌** | ✅ | 机器人 |

> **一句话总结**：Sora 证明了"视频生成 ≈ 世界模拟"这一重大洞察——扩展视频模型可以涌现物理理解。后续工作（iVideoGPT、Vid2World、ViPRA）则让视频世界模型从"被动生成"变为"可交互控制"，真正可以用于机器人规划和 RL 训练。
