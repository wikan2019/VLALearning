# WM-06. 自动驾驶世界模型

## 1. 为什么自动驾驶需要世界模型？

自动驾驶面临一个根本矛盾：**系统必须处理极端罕见的场景（corner cases），但这些场景在现实中几乎不会发生。**

```
常见场景:  直行、转弯、跟车         → 数据充足 ✅
极端场景:  逆行卡车、动物穿越、暴风雪 → 几乎无数据 ❌
                                          ↑
                                 但恰恰是这些场景决定了安全性！
```

**世界模型的价值**：生成逼真的极端驾驶场景，用于训练和测试自动驾驶系统。

## 2. 三大表示范式

| 范式 | 预测什么 | 优点 | 缺点 | 代表 |
| :--- | :--- | :--- | :--- | :--- |
| **VideoGen** | 多视角驾驶视频 | 直观、可视化 | 计算量大 | GAIA, DriveDreamer |
| **OccGen** | 3D 占用网格 | 几何精确 | 分辨率受限 | OccWorld |
| **LiDARGen** | 3D 点云序列 | 适合激光雷达系统 | 稀疏 | LiDARDM |

## 3. GAIA-1（2023.09，Wayve）

### (a) 核心能力

GAIA-1 是首个大规模生成式自动驾驶世界模型：

```
输入:  [视频帧] + [文本描述] + [驾驶动作]
                     ↓
        9B 参数的自回归 Transformer
                     ↓
输出:  未来驾驶视频（逼真的城市驾驶场景）
```

### (b) 学到的理解

GAIA-1 从纯驾驶视频中学会了：
*   **场景布局**：道路结构、车道线
*   **3D 几何**：深度、透视
*   **车辆动力学**：转弯时的惯性、加速减速
*   **交通规则**：红灯停、绿灯行（大部分时候）
*   **天气变化**：晴天、雨天、夜间的视觉差异

### (c) 可控生成

通过条件输入控制生成内容：
*   **文本控制**："下雨的夜晚" → 生成雨夜驾驶场景
*   **动作控制**："左转" → 生成左转的视角变化
*   **场景控制**："前方有行人" → 生成行人出现的场景

## 4. GAIA-2（2025.04，Wayve）

### (a) 升级点

| 特性 | GAIA-1 | GAIA-2 |
| :--- | :--- | :--- |
| **架构** | 自回归 Transformer | **潜在扩散模型 (Latent Diffusion)** |
| **视角** | 单视角 | **多摄像头视角** |
| **分辨率** | 中等 | **高分辨率** |
| **可控性** | 文本+动作 | **自车+他车+环境+道路语义** |
| **地域泛化** | 英国 | **英国+美国+德国** |

### (b) 精细控制

GAIA-2 支持极其精细的场景控制：
*   **自车 (Ego)**：速度、转向、加速度。
*   **他车 (Agents)**：每辆车的位置、速度、行为。
*   **环境**：天气、时间、光照。
*   **道路语义**：车道线、路牌、交通灯状态。

## 5. DriveDreamer（2024，ECCV）

### (a) 核心创新：多条件编码

```
输入条件:
  ├── 参考图像 (Reference Frame)
  ├── HD 地图 (车道线、路沿)
  ├── 3D 检测框 (其他车辆位置)
  └── 驾驶动作 (方向盘角度、油门)
       ↓
  多条件扩散模型
       ↓
输出: 视角一致、帧连贯的驾驶视频
```

### (b) 数据增强应用

DriveDreamer 的核心价值不在于"看视频"，而在于**生成训练数据**：

```
真实数据（有限）:  1000 小时驾驶视频
                      ↓
DriveDreamer 生成:    10000 小时合成视频 ← 10 倍数据增强
                      ↓
3D 检测模型训练:      用合成数据 + 真实数据 → 检测精度 +5-10%
```

## 6. OccWorld：3D 占用网格预测

### (a) 与视频世界模型的区别

```
视频世界模型:  预测 [RGB 像素序列] → 好看但几何不精确
OccWorld:      预测 [3D 占用网格] → 不好看但几何精确
```

```
3D 占用网格：将空间划分为 200×200×16 的体素网格
每个体素: 0 = 空 / 1 = 被占用 / 类别标签

  █░░░░░░░░█
  ██░░░░░░██   ← 前方车辆的体素表示
  ████████████   ← 道路表面
```

### (b) 优势

*   **碰撞检测精确**：体素级别的占用预测 → 精确判断"这个位置能不能过"。
*   **规划友好**：自动驾驶规划器可以直接在占用网格中搜索无碰撞路径。
*   **与激光雷达对齐**：占用网格天然与 LiDAR 点云兼容。

## 7. UniSim：统一仿真

**UniSim** 的目标是提供一个统一的可学习仿真器，通过世界模型替代传统物理引擎：

```
传统:    3D 资产 + 物理引擎 (Unreal/Unity) → 规则驱动的仿真
UniSim:  真实视频 + 神经网络 → 数据驱动的仿真
```

## 8. 各方法对比

| 模型 | 表示 | 多视角 | 可控条件 | 应用 |
| :--- | :--- | :--- | :--- | :--- |
| GAIA-1 | 视频 | ❌ 单视角 | 文本+动作 | 场景生成 |
| **GAIA-2** | 视频 | **✅ 多摄像头** | 自车+他车+环境+语义 | 多视角合成 |
| DriveDreamer | 视频 | ✅ 多视角 | 地图+3D框+动作 | **数据增强** |
| OccWorld | 占用网格 | N/A (3D) | 历史轨迹 | **规划** |
| UniSim | 视频 | ✅ | 布局+动作 | 统一仿真 |

## 9. 自动驾驶世界模型 vs 传统仿真

| 特性 | 传统仿真 (CARLA) | 世界模型 (GAIA-2) |
| :--- | :--- | :--- |
| **真实感** | 中（渲染器有限） | **高（从真实视频学习）** |
| **场景多样性** | 低（手工建模） | **高（可控生成）** |
| **物理精度** | **高（精确方程）** | 中（近似学习） |
| **扩展成本** | 高（每个场景都要建模） | 低（条件输入即可） |
| **极端场景** | 难以覆盖 | **容易生成** |

> **一句话总结**：自动驾驶世界模型解决了"极端场景数据稀缺"这一核心瓶颈——GAIA-2 可以生成任意条件下的多视角驾驶场景，DriveDreamer 将合成数据用于训练感知模型，OccWorld 提供几何精确的 3D 预测。它们正在从"辅助工具"变为自动驾驶开发的"核心基础设施"。
