# 36. 训练范式：模仿学习、强化学习与闭环训练

[⬅️ 返回 AD 目录](./README.md)

---

## 1. 行为克隆 (Behavior Cloning / IL)

### 基本思想

**行为克隆 (BC)** 是最简单的模仿学习方法——将驾驶建模为监督学习：给定观测，模仿人类动作。

$$\mathcal{L}_{\text{BC}} = \mathbb{E}_{(o, a^*) \sim \mathcal{D}} \left[ \| \pi_\theta(o) - a^* \|^2 \right]$$

其中 $o$ 是观测 (相机图像)，$a^*$ 是人类驾驶员的动作 (转向/油门/刹车)，$\pi_\theta$ 是策略网络。

```
训练流程:
────────────────────────────────
  人类驾驶数据集 D = {(o₁,a₁*), (o₂,a₂*), ...}

  对每个样本:
  ┌──────────┐        ┌──────────────┐
  │ 观测 oᵢ  │  ──→   │ 策略 πθ(oᵢ)  │ ──→ 预测动作 â
  └──────────┘        └──────────────┘
                              ↓
                      Loss = ||â - aᵢ*||²
                              ↓
                        反向传播更新 θ
```

### 协变量漂移 (Covariate Shift)

行为克隆最大的问题是**协变量漂移**——训练时看到的状态分布与推理时不同：

```
协变量漂移的直觉:
══════════════════════════════════════════════════════

训练数据 (人类驾驶):
  ──────→──────→──────→──────→──────→  (直线行驶)
  所有状态都在"正常轨迹"上

推理时 (模型自己驾驶):
  ──────→──────→─╲
                   ╲  ← 一个小误差
                    ╲
                     →──?? ← 到了从未见过的状态！
                           ← 模型不知道怎么回到正轨
                             ← 更大的误差
                               ← 偏离越来越远...

误差累积图:
                                        ██
                                      ████
                                    ██████
偏差 ▲                            ████████
     │                          ██████████
     │                    ██████████████
     │              ██████████████████
     │      ██████████████████████
     │████████████████████████
     └──────────────────────────────────→ 时间步
     t₀    t₁    t₂    t₃    t₄    t₅
```

数学解释：在时间步 $t$，误差上界为：

$$\epsilon_t \leq t \cdot \epsilon_{\text{single}} + \sum_{k=1}^{t-1} C^{t-k} \cdot \epsilon_k$$

其中 $C > 1$ 是误差放大系数。误差**指数增长**，而非线性增长。

### BC 的优劣

| 优势 | 劣势 |
|------|------|
| 实现简单，训练稳定 | 协变量漂移 |
| 数据需求可控 | 无法超越人类水平 |
| 可利用海量离线数据 | 多模态行为混乱 (同一场景人类可能左转或右转) |
| 收敛快 | 忽略时序因果关系 |

---

## 2. DAgger 与闭环修正

### DAgger (Dataset Aggregation)

DAgger 通过让**策略在真实环境/仿真中运行**，然后用专家标注策略访问的状态来解决协变量漂移：

```
DAgger 算法:
══════════════════════════════════════

第 1 轮: 用人类数据训练初始策略 π₁
         D₁ = 人类数据

第 2 轮: 运行 π₁ → 收集到状态 s₁, s₂, ...
         让专家在这些状态上标注: a₁*, a₂*, ...
         D₂ = D₁ ∪ {(s₁,a₁*), (s₂,a₂*), ...}
         训练 π₂ on D₂

第 3 轮: 运行 π₂ → 收集新状态
         专家标注 → D₃ = D₂ ∪ 新数据
         训练 π₃ on D₃

  ...重复直到收敛

关键: 训练数据覆盖了策略实际会访问的状态分布！
```

$$\mathcal{D}_{n+1} = \mathcal{D}_n \cup \{(s, \pi^*(s)) \mid s \sim d^{\pi_n}\}$$

其中 $d^{\pi_n}$ 是策略 $\pi_n$ 的状态访问分布，$\pi^*$ 是专家策略。

### 闭环微调 (Closed-Loop Fine-Tuning)

在仿真环境中运行策略，用闭环反馈来改进：

```
开环训练 (BC):                   闭环训练:
──────────────────              ──────────────────
数据集 → 训练 → 完成             数据集 → 预训练
                                      ↓
                                仿真器中运行策略
                                      ↓
                                收集闭环数据 + 奖励
                                      ↓
                                微调策略
                                      ↓
                                重复 ↑
```

### RoaD (2025) — 策略 Rollout 作为示范

RoaD 提出了一个巧妙的思路：**用策略自己的成功 rollout 作为新的训练示范**：

```
RoaD 流程:
┌──────────────────────────────────────────┐
│ 1. BC 预训练策略 π₀                       │
│ 2. 在仿真中运行 π₀ → 收集 rollout        │
│ 3. 筛选: 只保留"成功"的 rollout           │
│    (无碰撞 + 到达目标 + 舒适度达标)        │
│ 4. 用成功 rollout 作为新的训练数据         │
│ 5. 训练 π₁ → 运行 → 筛选 → 训练 π₂...   │
└──────────────────────────────────────────┘

结果: CARLA 基准上 41% 驾驶分数提升 (vs BC baseline)
```

| 属性 | RoaD |
|------|------|
| 核心创新 | 策略自己的成功 rollout 替代专家标注 |
| 不需要 | 专家在线标注 (DAgger 需要) |
| 不需要 | 奖励函数设计 (RL 需要) |
| 提升 | CARLA 驾驶分数 +41% |
| 年份 | 2025 |

---

## 3. 强化学习

### RL 用于自动驾驶

强化学习通过最大化**累积奖励**来学习驾驶策略：

$$\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right]$$

### 奖励函数设计

```
典型驾驶奖励:
─────────────────────────────
  r(s, a) =  w₁ · progress      (向目标前进)
           + w₂ · lane_keeping   (保持在车道内)
           + w₃ · speed_match    (接近限速)
           - w₄ · collision      (碰撞惩罚)
           - w₅ · jerk           (加速度变化率 → 舒适度)
           - w₆ · off_road       (驶出道路惩罚)
           - w₇ · traffic_viol   (违反交通规则)
```

### AD-R1 (2025) — 世界模型 RL + 反事实安全

AD-R1 将 RL 与**世界模型**结合，并创新性地引入**反事实安全场景**：

```
AD-R1 架构:
┌───────────────────────────────────────────────┐
│                                               │
│  当前观测 oₜ                                   │
│       ↓                                       │
│  ┌──────────────┐                             │
│  │ 驾驶世界模型  │ ← 生成未来场景               │
│  └──────┬───────┘                             │
│         │                                     │
│    ┌────┴────┐                                │
│    ↓         ↓                                │
│ 正常场景   反事实安全场景                        │
│ (前方空旷)  (如果前车急刹车呢?)                  │
│    ↓         ↓                                │
│ 正常奖励   安全奖励 (必须在反事实中也安全)        │
│    └────┬────┘                                │
│         ↓                                     │
│    综合奖励 → PPO 更新策略                      │
│                                               │
└───────────────────────────────────────────────┘
```

| 属性 | AD-R1 |
|------|-------|
| 核心创新 | 反事实安全场景 (counterfactual safety) |
| RL 算法 | PPO (Proximal Policy Optimization) |
| 世界模型 | 用于生成训练场景 (不需要仿真器) |
| 优势 | 考虑"最坏情况"，安全性更强 |
| 年份 | 2025 |

### RIFT (2025) — IL 预训练 → RL 微调

RIFT 提出了目前最有效的混合训练范式：先用模仿学习热启动，再用物理仿真中的 RL 微调。

```
RIFT 两阶段训练:
═══════════════════════════════════════

阶段 1: IL 预训练 (离线)
───────────────────────────
  大规模人类驾驶数据 → BC 训练
  → 获得一个"还不错"的初始策略 π₀
  → 但有协变量漂移问题

阶段 2: RL 微调 (闭环)
───────────────────────────
  把 π₀ 放入物理仿真器
  → 闭环运行 → 收集 (s, a, r) 元组
  → PPO / SAC 更新策略
  → 策略学会自我修正
  → 突破模仿学习的天花板

关键: 阶段 1 给出了好的初始化
      阶段 2 解决了协变量漂移
      → 1 + 2 > 单独使用任何一个
```

| 属性 | RIFT |
|------|------|
| 阶段 1 | IL 预训练 (大规模人类数据) |
| 阶段 2 | RL 微调 (物理仿真器) |
| 仿真器 | 基于物理引擎的闭环仿真 |
| RL 算法 | PPO |
| 优势 | IL 热启动 + RL 突破上限 |
| 年份 | 2025 |

### RAD (2025) — 3D Gaussian Splatting 用于 RL 环境

RAD 用 **3D Gaussian Splatting (3DGS)** 从真实驾驶数据重建出**照片级逼真的仿真环境**，用于 RL 训练：

```
RAD 流程:
┌─────────────────────────────────────────────────┐
│                                                 │
│  真实驾驶数据 → 3D Gaussian Splatting 重建       │
│                    ↓                            │
│              照片级逼真的 3D 场景                 │
│              (可自由渲染任意视角)                  │
│                    ↓                            │
│              作为 RL 环境:                       │
│              - 自车在场景中自由行动               │
│              - 实时渲染新视角                     │
│              - 碰撞检测 + 奖励计算               │
│                    ↓                            │
│              RL 训练 → 策略 π                    │
│                                                 │
└─────────────────────────────────────────────────┘

优势: 真实数据 → 真实感仿真 (无 sim-to-real gap)
```

---

## 4. 混合训练策略

### IL + RL 混合 (最常见范式)

```
当前业界最佳实践:
══════════════════════════════════════

Step 1: 大规模 IL 预训练
─────────────────────────
  数据: 百万级真实驾驶片段
  方法: BC (MSE loss) 或 生成式建模 (Flow Matching)
  目标: 获得合理的驾驶行为初始化
  耗时: 数天-数周 (千 GPU 级)

Step 2: RL 闭环微调
─────────────────────────
  环境: 物理仿真器 / 世界模型
  方法: PPO / SAC
  奖励: progress + safety - collision - jerk
  目标: 解决协变量漂移，提升鲁棒性
  耗时: 数天 (百 GPU 级)

Step 3: 持续学习 (可选)
─────────────────────────
  新数据: 车队运行中的困难样本
  方法: 增量训练 / Hard case replay
  目标: 覆盖长尾场景
  持续进行
```

### 世界模型作为 RL 环境

```
传统 RL:                          世界模型 RL:
────────────                      ───────────
策略 → 仿真器 → (s',r)            策略 → 世界模型 → (s',r)
  ↑      ↑                         ↑       ↑
  │  需要物理引擎                    │   神经网络 (可微分!)
  │  渲染开销大                      │   前向传播即可
  │  场景有限                        │   无限场景变化
```

用世界模型替代仿真器的优势：
- **可微分**：可以通过世界模型直接反向传播到策略
- **多样性**：通过条件生成产生无限场景变化
- **无 Sim-to-Real Gap**：世界模型从真实数据学习

### 课程学习 (Curriculum Learning)

```
难度递进:
──────────────────────────────────────────
Level 1: 直线道路 + 无交通           (简单)
    ↓
Level 2: 弯道 + 稀疏交通             (中等)
    ↓
Level 3: 路口 + 中等交通 + 交通灯    (较难)
    ↓
Level 4: 复杂路口 + 密集交通          (困难)
    ↓
Level 5: 恶劣天气 + 施工区 + 行人横穿  (极难)
    ↓
Level 6: 对抗场景 + 极端边缘情况      (挑战)
```

---

## 5. 数据引擎

### Tesla 数据飞轮

Tesla 的数据引擎是一个**自动化的闭环系统**，将部署、数据采集、训练连成循环：

```
Tesla 数据引擎详解:
═══════════════════════════════════════════════

      ┌──────────────────────────────────┐
      │         ① 模型部署 (OTA)          │
      │   将新模型推送到百万辆 Tesla       │
      └──────────────┬───────────────────┘
                     │
                     ↓
      ┌──────────────────────────────────┐
      │         ② 影子模式运行            │
      │   后台运行模型，与人类对比         │
      │   发现模型"犯错"的场景            │
      └──────────────┬───────────────────┘
                     │
                     ↓
      ┌──────────────────────────────────┐
      │         ③ 困难样本上传            │
      │   只上传模型表现差的片段           │
      │   → 高效数据采集 (不上传垃圾数据)  │
      └──────────────┬───────────────────┘
                     │
                     ↓
      ┌──────────────────────────────────┐
      │         ④ 自动标注                │
      │   多帧 SfM → 3D 重建 → 自动标签   │
      │   人工审核极少量 (<1%)             │
      └──────────────┬───────────────────┘
                     │
                     ↓
      ┌──────────────────────────────────┐
      │         ⑤ 训练新模型              │
      │   IL (新数据 + 旧数据)            │
      │   + RL (仿真/世界模型)            │
      └──────────────┬───────────────────┘
                     │
                     └──→ 回到 ① (循环)
```

### 其他公司的数据引擎

| 公司 | 数据来源 | 标注方式 | 训练闭环 |
|------|---------|---------|---------|
| **Tesla** | 百万辆消费者车辆 | 自动标注 + 影子模式 | OTA → 影子 → 上传 → 训练 → OTA |
| **Waymo** | ~1000 辆 Robotaxi | 人工 + 自动 | 路测 → 标注 → 训练 → 仿真验证 |
| **小鹏** | 消费者车辆 (40 万+) | 自动标注 + 人工 | OTA → 影子 → 训练 → OTA |
| **华为 ADS** | 合作车企车辆 | 自动标注 + 仿真 | 部署 → 采集 → 训练 → 验证 |

---

## 6. 训练范式对比表

| 维度 | 行为克隆 (BC/IL) | DAgger | 强化学习 (RL) | 世界模型 RL | 混合 (IL+RL) |
|------|----------------|--------|-------------|-----------|-------------|
| **数据需求** | 大量离线数据 | 需要在线专家 | 需要仿真器 | 需要世界模型 | 离线+在线 |
| **协变量漂移** | 严重 | 缓解 | 无 | 无 | 缓解 |
| **训练稳定性** | 高 | 中 | 低 | 中 | 中-高 |
| **能否超越人类** | 不能 | 不能 | 能 | 能 | 能 |
| **Sim-to-Real Gap** | 无 | 部分 | 严重 | 较小 | 较小 |
| **实现复杂度** | 低 | 中 | 高 | 高 | 高 |
| **代表方法** | UniAD, VAD | DAgger | PPO, SAC | AD-R1 | RIFT, Tesla FSD |
| **计算成本** | 中 | 中 | 高 | 高 | 最高 |
| **适用阶段** | 初始训练 | 增量改进 | 微调 | 微调 | 全流程 |

### 范式演进图

```
演进路径:
═══════════════════════════════════════════════

2015-2020: 纯行为克隆
    ↓ 协变量漂移是瓶颈
2020-2022: DAgger + 闭环数据
    ↓ 需要在线专家，不够自动化
2022-2024: IL + 仿真 RL
    ↓ Sim-to-Real Gap
2024-2025: IL + 世界模型 RL (AD-R1, RIFT)
    ↓ 世界模型质量是瓶颈
2025-2026: IL + RL + 推理 (Tesla FSD v13+)
    ↓ 大模型推理 + RL 探索
    ↓
未来: 自我进化——策略发现新场景 → 世界模型学习 → RL 优化 → 策略进化
```

---

## 7. 总结

**核心要点回顾**：

1. **行为克隆 (BC)**：简单有效的起点，但协变量漂移是根本瓶颈——误差在推理时指数增长
2. **DAgger / 闭环修正**：通过让策略运行并用专家纠正来缓解漂移；RoaD (2025) 用策略自己的成功 rollout 替代专家
3. **强化学习**：通过奖励驱动的探索可以超越人类水平；AD-R1 引入反事实安全，RIFT 证明 IL+RL 是最强范式
4. **世界模型 RL**：用神经网络替代物理仿真器，消除 sim-to-real gap，但受限于世界模型质量
5. **混合训练 (IL→RL)** 是当前最优范式：IL 提供好的初始化，RL 突破上限
6. **数据引擎**：Tesla 的飞轮效应证明了大规模闭环数据系统的威力

---

**关联阅读**：

- ⬅️ [35. Tesla FSD](./35_Tesla_FSD.md) — Tesla 的数据引擎实践
- ⬅️ [34. 驾驶世界模型](./34_Driving_World_Models.md) — 世界模型作为 RL 环境
- ➡️ [37. 安全验证](./37_Safety_Verification.md) — 训练好的模型如何验证安全
- 🔗 [VLA/Appendix A. Flow Matching](../VLA_Advances/Appendix_A_Flow_Matching.md) — 生成式训练方法

[⬅️ 返回 AD 目录](./README.md)
