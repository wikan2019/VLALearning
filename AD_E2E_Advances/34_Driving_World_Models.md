# 34. 驾驶世界模型：在脑中预演未来

[⬅️ 返回 AD 目录](./README.md)

---

## 1. 什么是驾驶世界模型

### 定义

**驾驶世界模型 (Driving World Model)** 是一种生成式神经网络，给定当前场景状态和未来动作，预测未来的驾驶视频帧——在"脑中"模拟驾驶过程：

$$\hat{V}_{t+1:t+T} = \mathcal{W}(V_{1:t}, \, a_{t:t+T-1}, \, c)$$

其中 $V_{1:t}$ 是历史视频帧，$a$ 是动作序列（方向盘角度、油门等），$c$ 是可选条件（文本指令、HD Map、3D 框等）。

### 直觉类比

```
人类驾驶员的"脑内模拟":
──────────────────────────────
  当前场景: 前方路口，左侧有车
       ↓
  想象动作 A: 直行通过
       → 脑补结果: 安全通过 ✓
  想象动作 B: 左转
       → 脑补结果: 与左侧车辆冲突 ✗
       ↓
  选择动作 A

驾驶世界模型做的就是同样的事——用神经网络"脑补"未来。
```

### 三大应用场景

```
                 ┌─────────────────────┐
                 │   Driving World     │
                 │      Model          │
                 └──┬──────┬──────┬───┘
                    │      │      │
          ┌─────────┤      │      ├─────────┐
          ↓         │      ↓      │         ↓
    ┌───────────┐   │ ┌─────────┐ │  ┌────────────┐
    │  规划决策   │   │ │数据增强  │ │  │ 闭环仿真    │
    │ Planning  │   │ │ Data    │ │  │ Simulation │
    │           │   │ │  Aug    │ │  │            │
    │ 模拟动作后果│   │ │生成稀有  │ │  │替代规则仿真器│
    │ 选最优轨迹 │   │ │ 场景    │ │  │ 更真实      │
    └───────────┘   │ └─────────┘ │  └────────────┘
                    │             │
                    └─────────────┘
```

| 应用 | 价值 | 例子 |
|------|------|------|
| **规划** | 在执行前"想象"后果 | 比较 5 条候选轨迹，选最安全的 |
| **数据增强** | 生成训练数据中缺少的场景 | 暴风雪、逆光、行人横穿 |
| **闭环仿真** | 替代规则仿真器（CARLA 等） | 真实渲染 + 可控场景 |

---

## 2. 代表模型

### (a) GAIA-1 (Wayve, 2023) — 自回归视频预测

GAIA-1 是首个大规模驾驶世界模型，用自回归 Transformer 生成未来视频帧。

```
架构:
┌───────────┐  ┌───────────┐  ┌──────────┐
│ Video     │  │ Text      │  │ Action   │
│ Encoder   │  │ Encoder   │  │ Encoder  │
│ (VQ-VAE)  │  │ (Frozen)  │  │ (MLP)    │
└─────┬─────┘  └─────┬─────┘  └─────┬────┘
      │              │              │
      └──────────┬───┘──────────────┘
                 ↓
      ┌─────────────────────┐
      │  Autoregressive      │
      │  World Model         │
      │  (9B params, GPT)    │
      │  next-token predict  │
      └──────────┬──────────┘
                 ↓
      ┌─────────────────────┐
      │  Video Decoder       │
      │  (VQ-VAE Decoder)    │
      └──────────┬──────────┘
                 ↓
         生成的未来视频帧
```

| 属性 | GAIA-1 |
|------|--------|
| 生成方式 | Autoregressive (next-token) |
| 参数量 | 9B |
| 条件输入 | 视频 + 文本 + 动作 |
| 视频编码 | VQ-VAE |
| 可控性 | 文本描述场景变化，动作控制轨迹 |
| 训练数据 | Wayve 内部驾驶数据 |

### (b) DriveDreamer (2024) — HD Map + 3D 框条件生成

DriveDreamer 将**结构化驾驶知识** (HD Map、3D 检测框) 作为显式条件注入生成过程。

```
条件注入:
┌──────────────┐    ┌───────────────┐    ┌──────────────┐
│  HD Map      │    │  3D Bounding  │    │  Text /      │
│  (BEV 线条)  │    │  Boxes        │    │  Action      │
└──────┬───────┘    └──────┬────────┘    └──────┬───────┘
       │                   │                    │
       └───────────┬───────┘────────────────────┘
                   ↓
       ┌──────────────────────┐
       │  Diffusion Model     │
       │  (Multi-View)        │
       │  条件 = Map + Box +  │
       │          Text/Action │
       └──────────┬───────────┘
                  ↓
       Multi-View 驾驶视频 (6 cameras)
```

| 属性 | DriveDreamer |
|------|-------------|
| 会议 | ECCV 2024 |
| 生成方式 | Diffusion (条件去噪) |
| 条件类型 | HD Map + 3D Box + Text + Action |
| 多视角 | 支持 6 相机一致生成 |
| 核心优势 | 结构化条件确保几何一致性 |

### (c) GAIA-2 (Wayve, 2025) — 潜空间扩散多视角世界模型

GAIA-2 是 GAIA-1 的继任者，用 **Latent Diffusion** 替代自回归，大幅提升生成质量和可控性。

```
GAIA-1 vs GAIA-2:
─────────────────────────────────────────────
GAIA-1:  VQ-VAE → Autoregressive GPT → Decode
         - 逐 token 生成，误差累积
         - 单视角

GAIA-2:  Video Encoder → Latent Diffusion → Decode
         - 并行去噪，质量更高
         - 多视角一致
         - 精细可控 (ego motion, 其他车辆, 天气)
─────────────────────────────────────────────
```

| 属性 | GAIA-2 |
|------|--------|
| 生成方式 | Latent Diffusion |
| 视角 | Multi-View (前/后/左/右) |
| 可控维度 | 自车运动、他车行为、天气、时间 |
| 用途 | 仿真 + 规划 + 数据增强 |
| 物理一致性 | 多视角几何约束 + 时序一致性损失 |

### (d) Drive-WM — 扩散世界模型用于规划

Drive-WM 将世界模型直接嵌入规划循环——用扩散模型预测多个候选动作的未来场景，然后选择最优动作。

```
规划循环:
  对每个候选动作 aᵢ:
  ┌─────────────────────────────────────────┐
  │  当前观测 oₜ + 候选动作 aᵢ              │
  │       ↓                                 │
  │  Drive-WM → 预测未来帧 V̂ₜ₊₁:ₜ₊ₖ       │
  │       ↓                                 │
  │  评估: 安全性? 进度? 舒适度?             │
  │       → Score(aᵢ)                       │
  └─────────────────────────────────────────┘
  选择 a* = argmax Score(aᵢ)
```

| 属性 | Drive-WM |
|------|----------|
| 核心思想 | 世界模型作为规划器的"想象引擎" |
| 生成方式 | Diffusion-based |
| 规划方式 | 生成 → 评估 → 选择 |
| 优势 | 不需要显式奖励函数，通过视觉预测评估安全 |

### (e) GenAD (Video Prediction) — 大规模视频预测

GenAD 训练于 **2000+ 小时**真实驾驶数据，是目前规模最大的驾驶视频预测模型之一。

| 属性 | GenAD (视频预测版本) |
|------|-------------------|
| 训练数据 | 2000+ 小时真实驾驶视频 |
| 输入 | 历史视频帧 + 导航指令 |
| 输出 | 未来 3-5 秒多视角视频 |
| 模型架构 | Diffusion Transformer (DiT) |
| 规模效应 | 数据量 ↑ → 视频质量和物理一致性 ↑ |

---

## 3. 世界模型的三大应用

### 应用 1: 规划——模拟动作后果

```
                    当前帧 + 候选动作
                         │
            ┌────────────┼────────────┐
            ↓            ↓            ↓
       动作 A: 直行   动作 B: 变道   动作 C: 刹车
            ↓            ↓            ↓
       ┌─────────┐  ┌─────────┐  ┌─────────┐
       │ 世界模型 │  │ 世界模型 │  │ 世界模型 │
       │ 预测未来 │  │ 预测未来 │  │ 预测未来 │
       └────┬────┘  └────┬────┘  └────┬────┘
            ↓            ↓            ↓
       安全通过 ✓    碰撞风险 ✗    安全但慢 △
            ↓
       选择动作 A ★
```

**优势**：不需要手动设计评估函数——未来视频帧本身就包含了所有信息。

### 应用 2: 数据增强——生成稀有场景

真实驾驶数据中，大多数场景是"正常行驶"，危险场景极少（长尾分布）：

```
场景分布:
频次 ▲
     │██████████████████████████████  正常行驶 (99%)
     │██
     │█                                变道切入 (0.5%)
     │▌                                紧急刹车 (0.3%)
     │▎                                行人横穿 (0.15%)
     │                                 逆光+暴雨 (0.01%)
     └────────────────────────────────→ 场景类型

世界模型可以根据文本条件生成:
  "暴风雪 + 高速路 + 前方车辆急刹" → 生成视频 → 用于训练
```

### 应用 3: 闭环仿真——替代规则仿真器

```
传统闭环仿真:                          世界模型闭环仿真:
┌─────────────────┐                  ┌─────────────────┐
│ CARLA / SUMO    │                  │ World Model     │
│ 规则渲染引擎     │                  │ 神经网络渲染     │
│ + 规则交通流     │                  │ + 学习的交通行为 │
│                 │                  │                 │
│ 优: 精确物理    │                  │ 优: 真实感更强   │
│ 缺: 渲染不真实  │                  │     交通行为更自然│
│     行为脚本化  │                  │ 缺: 物理不完美   │
└─────────────────┘                  └─────────────────┘
```

---

## 4. 挑战

### (1) 物理一致性 (Physical Consistency)

生成的视频必须遵守物理定律——车辆不能穿墙、不能悬浮、不能突然消失。

```
物理不一致的典型问题:
─────────────────────────────
✗ 车辆穿过建筑物             (碰撞违反)
✗ 路面纹理在帧间跳变          (时序不一致)
✗ 远处物体突然出现/消失        (遮挡逻辑错误)
✗ 车辆转弯半径违反运动学       (Ackermann 约束违反)
✗ 多视角间同一物体位置不一致    (几何不一致)
```

**缓解方法**：
- 多视角几何约束 (epipolar attention)
- 物理先验损失 (motion dynamics loss)
- 3D 结构化条件 (HD Map + 3D Box)

### (2) 长期预测稳定性

```
预测质量 vs 预测时长:

质量 ▲
     │████████
     │  ████████
     │     ████████
     │        ████████
     │           ███████
     │              ████
     │                 █     ← 超过 5 秒后质量急剧下降
     └──────────────────────→ 预测时长
     0s   2s   4s   6s   8s
```

原因：误差在时间轴上**累积 (compounding error)**——每一帧的微小误差被下一帧放大。

### (3) 计算成本

| 模型 | 参数量 | 生成速度 | GPU 需求 |
|------|--------|---------|---------|
| GAIA-1 | 9B | ~2 FPS | 8× A100 |
| DriveDreamer | ~1B | ~5 FPS | 4× A100 |
| GAIA-2 | 未公开 | ~3 FPS | 多卡集群 |
| Drive-WM | ~500M | ~8 FPS | 2× A100 |

对于实时规划 (10+ Hz)，当前世界模型**远不够快**。潜空间操作 (Latent Space) 和模型蒸馏是主要加速方向。

---

## 5. 与机器人世界模型的对比

驾驶世界模型和机器人世界模型 (如 UniSim, Genie 2) 有共同的技术根基，但面临不同约束：

| 维度 | 驾驶世界模型 | 机器人世界模型 |
|------|------------|-------------|
| **动作空间** | 2-3 DoF (转向/油门/刹车) | 6-14 DoF (关节角度) |
| **视角** | 多相机环视 (6-8 cameras) | 通常 1-3 相机 |
| **安全要求** | 极高 (关乎生命) | 较低 (可恢复) |
| **物理约束** | 车辆运动学 (Ackermann) | 机械臂运动学 |
| **场景复杂度** | 开放道路，无限场景 | 通常室内/桌面 |
| **数据来源** | 车队采集 (十亿英里级) | 遥操作采集 (百万轨迹级) |
| **代表模型** | GAIA-1/2, DriveDreamer | UniSim, Genie 2 |

> **深入阅读**：→ [VLA/24. 世界模型与规划](../VLA_Advances/24_World_Models_Planning.md)

---

## 6. 总结

```
演进路径:

基于规则的仿真 (CARLA, 2017-)
    ↓ 渲染不真实，行为脚本化
神经网络渲染 (NeRF, 2020-)
    ↓ 静态场景，无动态交互
自回归世界模型 (GAIA-1, 2023)
    ↓ 误差累积，单视角
扩散世界模型 (DriveDreamer, Drive-WM, 2024)
    ↓ 多视角，结构化条件
大规模潜扩散世界模型 (GAIA-2, GenAD, 2025)
    ↓ 高保真，可控，多视角一致
    ↓
未来: 实时世界模型 + 物理引擎融合
```

**核心要点回顾**：

1. **什么是驾驶世界模型**：给定当前场景+动作，预测未来驾驶视频的生成式模型
2. **三大应用**：规划决策（模拟后果）、数据增强（生成稀有场景）、闭环仿真（替代规则引擎）
3. **技术演进**：从自回归 (GAIA-1) 到潜空间扩散 (GAIA-2)，精度和可控性不断提升
4. **核心挑战**：物理一致性、长期稳定性、计算成本
5. **与机器人世界模型**的共同技术根基与不同应用约束

---

**关联阅读**：

- ⬅️ [33. VLM/VLA 驾驶](./33_VLM_Driving.md) — 大模型驱动的驾驶决策
- ⬅️ [31. 占据网络](./31_Occupancy_Networks.md) — 世界模型的 3D 表示基础
- ➡️ [35. Tesla FSD](./35_Tesla_FSD.md) — 最大规模的端到端驾驶部署
- ➡️ [36. 训练范式](./36_Training_Paradigms.md) — 世界模型 RL 训练
- 🔗 [VLA/24. 世界模型与规划](../VLA_Advances/24_World_Models_Planning.md) — 机器人世界模型

[⬅️ 返回 AD 目录](./README.md)
