# 35. Tesla FSD：从规则到端到端神经网络

[⬅️ 返回 AD 目录](./README.md)

---

## 1. FSD 的演进路线

Tesla 的自动驾驶技术经历了**四个阶段**，从纯 C++ 规则引擎演进到端到端神经网络：

```
Phase 1 (2016-2020)          Phase 2 (2021-2023)
C++ 规则引擎                  HydraNets + BEV + 占据
┌──────────────────┐         ┌──────────────────────────┐
│ 相机 + 雷达      │         │ 相机 (移除雷达)           │
│      ↓          │         │      ↓                   │
│ 独立感知 CNN     │         │ HydraNets (多任务)       │
│      ↓          │         │      ↓                   │
│ C++ 规则规划器   │         │ BEV → Occupancy Network  │
│      ↓          │         │      ↓                   │
│ 控制输出        │         │ C++ 规则规划器 (改进)      │
└──────────────────┘         └──────────────────────────┘

Phase 3 (2024-2025)          Phase 4 (2026+)
FSD v12/v13 端到端            推理 AI + RL
┌──────────────────────┐     ┌──────────────────────────┐
│ 8 相机 36Hz 全分辨率  │     │ 8 相机 + 更高分辨率       │
│      ↓               │     │      ↓                   │
│ 端到端神经网络        │     │ 10x 更大的端到端模型      │
│ (photon-to-control)  │     │ + 推理能力 (Reasoning AI) │
│      ↓               │     │ + 强化学习 (RL)           │
│ 转向 / 油门 / 刹车   │     │      ↓                   │
└──────────────────────┘     │ 直接控制输出              │
                             └──────────────────────────┘
```

### 各阶段关键里程碑

| 阶段 | 时间 | 架构 | 传感器 | 规划方式 | 里程碑 |
|------|------|------|--------|---------|--------|
| Phase 1 | 2016-2020 | 独立 CNN | 相机+雷达 | C++ 规则 | Autopilot / Navigate on Autopilot |
| Phase 2 | 2021-2023 | HydraNets | 纯视觉 | BEV + C++ 规则 | FSD Beta v10/v11, Occupancy Network |
| Phase 3 | 2024-2025 | 端到端 NN | 纯视觉 | 神经网络 | FSD v12 (2024.01), v13 (2025.02) |
| Phase 4 | 2026+ | 端到端 + 推理 | 纯视觉 | NN + RL + 推理 | FSD v13.x+, Robotaxi |

---

## 2. FSD v12/v13 端到端架构

### 核心理念：Photon-to-Control

FSD v12 的革命性转变：**从相机接收的光子直接到控制信号输出**——中间没有任何手写的规则代码。

```
FSD v11 (Phase 2):
───────────────────────────────────────────────
相机 → [感知 NN] → 检测框 → [C++ 规则规划器] → 控制
  ↑ 神经网络      ↑ 手工中间表示    ↑ ~30 万行 C++ 代码

FSD v12/v13 (Phase 3):
───────────────────────────────────────────────
相机 → [端到端神经网络] → 控制 (转向/油门/刹车)
  ↑                    ↑
  光子直接到控制          没有手工规则代码
```

### 输入处理

```
8 cameras × 36 Hz × 全分辨率 (1280×960)
    │
    ↓
┌────────────────────────────────────────────┐
│  Video Encoder (ViT / 自研架构)             │
│  - 空间特征提取                              │
│  - 时序建模 (跨帧 attention)                 │
│  - 多相机融合                               │
│                                            │
│  输入: 连续多帧 (temporal context window)    │
│        不仅看当前帧，还看历史帧               │
└────────────────────┬───────────────────────┘
                     ↓
              统一时空特征表示
```

### 端到端网络结构

```
┌──────────────────────────────────────────────┐
│                                              │
│    8 Camera Video Streams (36 Hz)            │
│              ↓                               │
│    ┌─────────────────────┐                   │
│    │  Video Backbone     │                   │
│    │  (空间-时序编码)     │                   │
│    └──────────┬──────────┘                   │
│               ↓                              │
│    ┌─────────────────────┐                   │
│    │  Spatio-Temporal    │                   │
│    │  Feature Fusion     │                   │
│    │  (BEV / 3D 特征空间)│                   │
│    └──────────┬──────────┘                   │
│               ↓                              │
│    ┌─────────────────────┐  ┌──────────────┐ │
│    │  Planning Head      │  │ Navigation   │ │
│    │  (Transformer)      │←─│ Route Input  │ │
│    │  预测未来轨迹        │  └──────────────┘ │
│    └──────────┬──────────┘                   │
│               ↓                              │
│    ┌─────────────────────┐                   │
│    │  Control Output     │                   │
│    │  steering, throttle,│                   │
│    │  brake              │                   │
│    └─────────────────────┘                   │
│                                              │
└──────────────────────────────────────────────┘
```

### 训练方式

$$\mathcal{L} = \underbrace{\mathcal{L}_{\text{IL}}}_{\text{模仿人类驾驶}} + \lambda \cdot \underbrace{\mathcal{L}_{\text{RL}}}_{\text{强化学习奖励}}$$

- **模仿学习 (IL)**：从数百万段人类驾驶视频片段中学习——模型输出应尽量接近人类驾驶行为
- **强化学习 (RL)**：在仿真/世界模型中优化安全性和舒适度指标
- **训练规模**：
  - 数百万段视频片段 (每段 ~30 秒)
  - 数千 GPU 训练数周
  - v13 相比 v12: 4.2× 数据量、5× 算力

---

## 3. 数据引擎 (Data Engine)

Tesla 数据引擎是其最大的竞争壁垒——一个**闭环自动改进系统**：

```
                   Tesla 数据引擎 (Data Flywheel)
                   ──────────────────────────────
                            ┌──────────┐
                 ┌─────────→│  部署到车队 │──────────┐
                 │          │ (百万辆车) │          │
                 │          └──────────┘          │
                 │                                │
           ┌───────────┐                   ┌──────────┐
           │ 训练新模型  │                   │ 采集数据  │
           │ (IL + RL)  │                   │ 影子模式  │
           └─────┬─────┘                   └─────┬────┘
                 │                                │
                 │          ┌──────────┐          │
                 └──────────│ 自动标注  │←─────────┘
                            │ + 筛选   │
                            └──────────┘
```

### 影子模式 (Shadow Mode)

```
正常驾驶中:
┌─────────────────────────────────────────────┐
│  人类驾驶员控制车辆 (正常驾驶)                 │
│       ↓                                     │
│  同时，FSD 模型在后台运行（不控制车辆）         │
│       ↓                                     │
│  比较: 模型预测 vs 人类实际操作                 │
│       ↓                                     │
│  不一致? → 上传该片段到 Tesla 服务器           │
│  一致?   → 跳过，不上传                       │
└─────────────────────────────────────────────┘
```

这让 Tesla 能**自动发现模型的弱点**——只上传模型会犯错的场景。

### 自动标注流水线 (Auto-Labeling Pipeline)

```
原始相机视频
    ↓
多帧重建 (Structure from Motion)
    ↓
3D 场景重建 → 精确的 3D 位置和轨迹
    ↓
自动生成标注:
  - 3D 检测框 (车辆/行人/...)
  - 深度图
  - 占据网格
  - 可行驶区域
  - 交通信号状态
    ↓
质量过滤 → 高置信度标注进入训练集
```

### 困难样本挖掘 (Hard Case Mining)

| 挖掘策略 | 说明 | 示例 |
|---------|------|------|
| **影子模式不一致** | 模型预测 ≠ 人类行为 | 模型想直行但人类紧急刹车 |
| **高 loss 样本** | 模型在该片段上损失值高 | 复杂路口、遮挡场景 |
| **介入事件** | 人类从 FSD 接管控制 | 模型行为不当，人类干预 |
| **碰撞/近碰** | 极端危险场景 | 前方急刹车、行人突出 |
| **特殊场景** | 罕见但重要的驾驶场景 | 施工区域、紧急车辆 |

### 车队学习 (Fleet Learning)

```
全球 Tesla 车队规模:
─────────────────────────
  > 7,000,000,000 英里真实驾驶数据 (截至 2025)
  > 6,000,000 辆搭载 FSD 硬件的车辆
  每天新增 ~100,000,000 英里数据

对比:
  Waymo:  ~50,000,000 英里 (实际路测 + 仿真)
  Cruise: ~20,000,000 英里
  百度:   ~50,000,000 英里
```

---

## 4. 关键指标

### FSD v12 → v13 提升

| 指标 | FSD v12 | FSD v13 | 提升 |
|------|---------|---------|------|
| 训练数据量 | 基准 | 4.2× | 数据规模 |
| 训练算力 | 基准 | 5× | 更多 GPU/更长训练 |
| 推理延迟 | 基准 | 0.5× (减半) | 更快响应 |
| 人类介入频率 | ~100 英里/次 | ~300+ 英里/次 | 3× 更少介入 |
| 支持场景 | 城市 + 高速 | 城市 + 高速 + 停车场 | 更多场景 |

### 2026 更新方向

```
FSD v13.x+ (2026):
─────────────────────────────
  ┌─────────────────────────────────────┐
  │  10× 更大的模型                       │
  │       ↓                             │
  │  推理 AI (Reasoning):                │
  │    - 不仅"反射性"驾驶，还能"思考"      │
  │    - 复杂场景的多步推理               │
  │    - "如果前方有施工，应该提前变道"     │
  │       ↓                             │
  │  强化学习 (RL):                      │
  │    - 在世界模型/仿真中学习最优策略      │
  │    - 超越人类模仿的上限               │
  │       ↓                             │
  │  目标: 接近 L4 自动驾驶               │
  └─────────────────────────────────────┘
```

---

## 5. Tesla 方法的优劣分析

### 优势

```
优势矩阵:
───────────────────────────────────────────────────
  ┌───────────────────┐
  │ 1. 数据规模无敌    │  70 亿+ 英里真实数据
  │    (Data Scale)    │  > 所有竞争对手之和
  └───────────────────┘
  ┌───────────────────┐
  │ 2. 纯视觉 = 低成本│  无 LiDAR ($0 vs $10,000+)
  │    (Vision Only)   │  消费级硬件可量产
  └───────────────────┘
  ┌───────────────────┐
  │ 3. 车队飞轮效应    │  部署 → 数据 → 训练 → 部署
  │    (Fleet Flywheel)│  每天自动改进
  └───────────────────┘
  ┌───────────────────┐
  │ 4. 端到端简洁      │  删除 30 万行 C++ 规则
  │    (E2E Simplicity)│  一个神经网络搞定一切
  └───────────────────┘
  ┌───────────────────┐
  │ 5. 硬件-软件垂直整合│  自研 FSD 芯片 + Dojo 训练
  │    (Vertical Stack) │  端到端优化
  └───────────────────┘
```

### 劣势与争议

| 劣势 | 说明 | 影响 |
|------|------|------|
| **纯视觉局限** | 无 LiDAR 冗余，恶劣天气 (暴雨/大雾/强逆光) 下可靠性存疑 | 安全性 |
| **无形式化验证** | 端到端神经网络是"黑箱"，无法数学证明安全性 | 监管合规 |
| **监管挑战** | 多地面临法规限制；Autopilot 事故引发监管审查 | 商业化 |
| **过度承诺** | 多次承诺 L5 时间表未兑现 (2017: "明年 L5") | 信誉 |
| **闭源** | 架构细节未公开，学术界难以复现和验证 | 可验证性 |

### 与其他方案的对比

| 维度 | Tesla FSD | Waymo | 华为 ADS | 小鹏 XNGP |
|------|-----------|-------|---------|-----------|
| **传感器** | 纯视觉 | LiDAR+相机+雷达 | LiDAR+相机+雷达 | LiDAR+相机 |
| **架构** | 端到端 NN | 模块化 + ML | 模块化 → 端到端 | 端到端 |
| **数据规模** | 70 亿+ 英里 | ~5000 万英里 | 未公开 | 未公开 |
| **部署车辆** | ~600 万 | ~1000 (Robotaxi) | ~10 万+ | ~40 万+ |
| **运营模式** | 消费者 OTA | Robotaxi 车队 | 消费者 OTA | 消费者 OTA |
| **开放程度** | 闭源 | 部分论文 | 闭源 | 闭源 |
| **L4 进展** | Robotaxi 计划中 | 已运营 L4 Robotaxi | L4 示范运营 | L4 规划中 |

---

## 6. 对行业的启示

### 端到端转型的行业趋势

Tesla FSD v12 的成功验证了端到端路线的可行性，推动了整个行业的范式转变：

```
2023:  大多数公司 → 模块化 (感知 + 规则规划)
           │
           │ Tesla FSD v12 发布 (2024.01)
           │ 证明端到端可以工作
           ↓
2024:  行业加速转向端到端
       - 华为 ADS 3.0: 端到端架构
       - 小鹏 XNGP: 端到端升级
       - Waymo: Foundation Model 研究
       - 理想: 端到端 + VLM
           │
           ↓
2025-26: 端到端成为行业共识
       - 竞争焦点: 数据规模 + 训练方法 + 算力
```

### 关键经验

1. **数据是核心壁垒**：模型架构可以模仿，但 70 亿英里的数据引擎无法复制
2. **端到端不是魔法**：仍需要精心设计的数据引擎和训练策略
3. **安全验证是瓶颈**：技术可行不等于可以合法上路
4. **Vision-only 是勇敢的赌注**：降低成本但增加技术难度

---

**关联阅读**：

- ⬅️ [34. 驾驶世界模型](./34_Driving_World_Models.md) — 世界模型仿真
- ⬅️ [28. 架构概述](./28_AD_Overview.md) — 模块化 vs 端到端
- ➡️ [36. 训练范式](./36_Training_Paradigms.md) — Tesla 数据引擎的训练方法
- ➡️ [37. 安全验证](./37_Safety_Verification.md) — 端到端模型的安全验证挑战

[⬅️ 返回 AD 目录](./README.md)
