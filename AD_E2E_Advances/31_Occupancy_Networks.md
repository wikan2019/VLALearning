# 31. 3D 占据网络：从检测框到体素化世界

[⬅️ 返回 AD 目录](./README.md)

---

## 1. 为什么需要占据网络？

### 3D 检测框的局限

传统 3D 检测只输出**有限类别的边界框**（car, pedestrian, cyclist, …）。但真实路面远比这复杂：

```
3D 检测框视角:                          真实场景:
┌─────────┐                            ╔══════╗
│  🚗 car │  ← 已知类别，有框          ║ 倒伏 ║
└─────────┘                            ║ 大树 ║  ← 不在类别表中，没有框！
┌───────┐                              ╚══════╝
│🚶 ped │                              ┌~~~~~┐
└───────┘                              │散落的│  ← 施工碎石？翻倒的货物？
                                       │杂物  │
                                       └~~~~~┘
→ 类别表外的障碍物直接"消失"              → 安全隐患！
```

**核心问题**：施工路障、倒伏的树木、翻覆的卡车、路面散落物……这些都不在预定义类别中，3D 检测会**完全忽略**它们。

### 占据网络的解决方案

**Occupancy Network** 将三维空间划分为**密集体素网格 (Dense Voxel Grid)**，每个体素标记为：

| 状态 | 含义 | 规划意义 |
|------|------|---------|
| **Occupied** | 被物体占据 | 不可通行 |
| **Free** | 空闲空间 | 可通行 |
| **Unknown** | 未观测到 | 需保守处理 |

```
  3D 检测框 vs 占据体素
  ─────────────────────────

  检测框表示不规则障碍:        占据体素表示不规则障碍:

    ┌───────────────┐           ■ ■ □ □ □
    │               │           ■ ■ ■ □ □
    │   /‾‾‾\      │           ■ ■ ■ ■ □
    │  | 倒树 |     │    →      ■ ■ ■ ■ ■
    │   \___/       │           □ ■ ■ ■ □
    └───────────────┘           □ □ ■ □ □
    ↑ 框太大，包含很多空闲空间     ↑ 精确刻画形状，■=occupied □=free
```

> **关键优势**：占据网络**不需要预定义类别**，只要空间被物体占据就会标注——真正做到"通用障碍物表示"。

---

## 2. 占据网络的基本原理

### 输入与输出

```
输入:                              输出:
┌─────────────────┐               ┌─────────────────────────┐
│ 环视相机图像     │               │  3D 体素网格             │
│ (6-8 cameras)   │    Occupancy  │  200 × 200 × 16 voxels  │
│                 │ →  Network  → │  每个体素:               │
│ [可选] LiDAR    │               │   - 占据状态             │
│ [可选] Radar    │               │   - 语义类别             │
└─────────────────┘               └─────────────────────────┘
```

### 典型参数

| 参数 | 典型值 | 说明 |
|------|-------|------|
| 体素网格大小 | 200×200×16 | X, Y, Z 方向的体素数 |
| 感知范围 | [-40m, 40m] × [-40m, 40m] | 前后左右各 40m |
| 高度范围 | [-1m, 5.4m] | 地面以下 1m 到空中 5.4m |
| 体素分辨率 | 0.4m × 0.4m × 0.4m | 每个体素的物理尺寸 |
| 语义类别数 | 16-18 | 含 free 类 |

### 两条技术路线

```
路线 A: 3D Voxel Query + Cross-Attention
───────────────────────────────────────
  2D 图像特征 ─────────────────────────────────┐
                                               │ Cross-Attention
  3D 体素查询 (learnable queries) ─────────────┤
  [V×V×H 个 queries]                           │
                                               ↓
                                        3D 占据预测

路线 B: BEV → Lift to 3D
───────────────────────────────────────
  2D 图像特征 → BEV Encoder → BEV 特征 (H×W×C)
                                  ↓
                          Channel-to-Height
                          或 3D Deconvolution
                                  ↓
                          3D 占据预测 (H×W×Z×Classes)
```

**路线 A** 精度更高但计算量大（OccFormer, SurroundOcc）；**路线 B** 速度快、易部署（FlashOcc, FastOcc）。

---

## 3. 代表方法

### (a) TPVFormer — 三面投影视图

**核心思想**：完整的 3D 体素太占内存（200³ ≈ 800 万体素），用三个正交的 BEV 平面近似：

```
             ┌──────────┐
            /│          /│
           / │   XZ 面 / │
          /  │ (侧视) /  │
         ┌──────────┐   │
         │   │      │   │     TPV = Tri-Perspective View
         │   └──────│───┘
         │  / XY 面 │  /      三个平面的特征交汇 → 恢复 3D 信息
         │ / (俯视) │ /
         └──────────┘
          YZ 面 (正视)
```

| 属性 | 说明 |
|------|------|
| 表示方式 | 3 个 BEV 平面: XY (H×W), XZ (H×D), YZ (W×D) |
| 内存效率 | 3×(H×W) vs H×W×D，节省约 **16×** 内存 |
| 3D 恢复 | 任意 (x,y,z) 处的特征 = XY[x,y] + XZ[x,z] + YZ[y,z] |
| 来源 | CVPR 2023, Tsinghua & NVIDIA |

### (b) SurroundOcc — 多尺度 3D 反卷积

```
Coarse Voxel (25×25×2) → 3D Deconv → 50×50×4 → 3D Deconv → 100×100×8 → 3D Deconv → 200×200×16
       ↑                                ↑                        ↑
  Image Cross-Attn             Multi-scale Fusion         Fine-grained Prediction
```

- 从粗到细逐步上采样，每层融合不同尺度的图像特征
- 输出**密集语义占据**：每个体素不仅有 occupied/free，还有 17 类语义（car, road, sidewalk, vegetation …）
- nuScenes-Occupancy 基准上的早期 SOTA

### (c) FlashOcc — 实时占据预测

**核心创新**：Channel-to-Height 变换，避免真正的 3D 卷积。

```
传统方法:  BEV (H×W×C) → 3D Voxel (H×W×Z×Cls) → 3D Conv (慢!)
                                                    ↓
FlashOcc:  BEV (H×W×C) → Reshape → (H×W×Z×Cls)    直接把通道维度
           其中 C = Z × Cls                         拆分为高度×类别
           → 无需 3D Conv，纯 2D 操作！
```

| 指标 | SurroundOcc | FlashOcc |
|------|-------------|----------|
| mIoU (nuScenes) | 34.7 | 32.0 |
| 推理速度 | ~3 FPS | **>30 FPS** |
| 3D 卷积 | 需要 | **不需要** |
| 部署友好度 | 低 | **高** |

### (d) GS-Occ3D (2025) — 高斯泼溅驱动

**核心思想**：借鉴 3D Gaussian Splatting (3DGS) 的场景表示，用高斯体代替体素。

```
场景分解:
┌──────────────────────────────────────┐
│  Static Background    (建筑、树木)    │ → Background Gaussians (长期不变)
│  Ground Plane         (道路、地面)    │ → Ground Gaussians (平面约束)
│  Dynamic Objects      (车辆、行人)    │ → Dynamic Gaussians (带运动向量)
└──────────────────────────────────────┘
         │
         ↓ Gaussian Splatting Rendering
  渲染出 Occupancy + Semantic Labels
```

| 属性 | GS-Occ3D |
|------|----------|
| 场景表示 | 3D Gaussian Splatting |
| 分解策略 | 静态背景 + 地面 + 动态物体 |
| 数据集 | Waymo Open Dataset |
| 成果 | 2025 Waymo Occ3D SOTA |
| 优势 | 连续表示，不受体素分辨率限制 |

---

## 4. 占据流预测 (Occupancy Flow)

仅知道**当前**占据是不够的——规划器需要知道障碍物**将要去哪里**。

### 占据流 = 占据 + 运动预测

```
时刻 t:                        时刻 t+1 (预测):

■ ■ □ □ □ □ □                 □ □ ■ ■ □ □ □
■ ■ ■ □ □ □ □    flow →→      □ ■ ■ ■ □ □ □
□ ■ ■ □ □ □ □    (2,0)        □ □ ■ ■ □ □ □
   [移动的车辆]                    [车辆向右移动了 2 格]
```

每个被占据的体素附带一个 **flow vector (dx, dy, dz)**，预测该体素在未来 dt 时间内的位移。

### 数学表示

对于体素位置 $\mathbf{p} = (x, y, z)$，在时刻 $t$：

$$\mathbf{p}_{t+\Delta t} = \mathbf{p}_t + \mathbf{f}(\mathbf{p}_t)$$

其中 $\mathbf{f}(\mathbf{p}_t) \in \mathbb{R}^3$ 是模型预测的 flow vector。

### UniAD 中的 OccFormer

UniAD 使用 **OccFormer** 模块同时预测占据和占据流：

```
BEV Features → OccFormer → Occupancy (t, t+1, ..., t+T)
    ↓                   ↘ Occ Flow  (t→t+1, ..., t+T-1→t+T)
Track Queries              ↓
    ↓               Planner 碰撞检测:
Motion Forecast     轨迹点是否与未来占据重合？
    ↓                 → 重合 → 拒绝该轨迹
  Planner ←──────────→ 无重合 → 可行轨迹
```

---

## 5. 在端到端驾驶中的作用

### 占据作为安全层

在端到端规划中，占据预测充当**安全约束层 (Safety Constraint Layer)**：

```
感知模块                   规划模块
┌──────────┐             ┌──────────────────┐
│ 相机图像  │             │ 候选轨迹生成       │
│    ↓     │             │  τ₁, τ₂, ..., τₖ │
│ BEV 特征  │ ──────→     │       ↓           │
│    ↓     │             │ 碰撞检测           │
│ 3D 占据   │ ──────→     │  τᵢ ∩ Occ ≠ ∅ ?  │
│ + 占据流  │             │  Yes → 丢弃 τᵢ    │
└──────────┘             │  No  → 保留 τᵢ    │
                         │       ↓           │
                         │ 选择最优轨迹 τ*    │
                         └──────────────────┘
```

### 碰撞检测逻辑

对于候选轨迹 $\tau = \{(x_t, y_t)\}_{t=1}^{T}$，检查每个轨迹点在未来占据网格中的状态：

$$\text{collision}(\tau) = \bigcup_{t=1}^{T} \mathbb{1}\left[\text{Occ}(x_t, y_t, z_{\text{ego}}, t) = \text{occupied}\right]$$

若 $\text{collision}(\tau) = 1$，该轨迹被拒绝。

### 与其他表示的对比

| 表示方式 | 形状精度 | 未知类别 | 计算成本 | 规划适用性 |
|---------|---------|---------|---------|-----------|
| 3D 检测框 | 低（矩形近似） | 无法处理 | 低 | 一般 |
| BEV 分割 | 中（无高度） | 部分处理 | 中 | 中 |
| **3D 占据** | **高（体素级）** | **完全处理** | **高** | **最优** |
| 点云原始表示 | 最高 | 处理 | 最高 | 低（稀疏） |

---

## 6. 关键挑战

### (1) Ground Truth 生成昂贵

```
LiDAR 单帧:                    累积多帧后:
  ·  ·                           ·····
  ·    ·    ← 稀疏！             ·····  ← 稠密了
  ·  ·                           ·····
```

占据标注需要**多帧 LiDAR 点云累积 + 人工修正**，一个场景的标注成本远高于 2D 框标注。

| 标注方式 | 每帧成本 | 数据来源 |
|---------|---------|---------|
| 2D 检测框 | ~$0.1 | 人工标注 |
| 3D 检测框 | ~$1-5 | LiDAR + 人工 |
| **3D 占据** | **~$10-50** | **多帧 LiDAR 累积 + 语义分割 + 人工修正** |

### (2) 计算与内存瓶颈

200×200×16 × 18 类 = **~1150 万**个预测值。完整的 3D 卷积在嵌入式平台上几乎不可行。

**缓解策略**：
- **稀疏卷积** (Sparse Convolution)：只在非空体素上计算
- **Channel-to-Height** (FlashOcc)：避免 3D 卷积
- **TPV 投影** (TPVFormer)：用三平面代替全 3D
- **Gaussian Splatting** (GS-Occ3D)：连续表示替代离散网格

### (3) 远距离可靠性

```
距离与占据精度:

精度 ▲
100% │████
     │████████
     │████████████
     │████████████████
     │████████████████████
  0% └───────────────────────→ 距离
     0m   20m   40m   60m   80m

→ 超过 40-50m，相机像素分辨率不足 + LiDAR 点云稀疏
   → 占据预测质量急剧下降
```

---

## 7. 总结与展望

```
演进路径:

3D 检测框 (2018-)
    ↓ 只能表示已知类别
3D 占据 (2023-)
    ↓ 密集但计算量大
稀疏/高效占据 (FlashOcc, 2024)
    ↓ 可部署但分辨率有限
高斯占据 (GS-Occ3D, 2025)
    ↓ 连续表示 + 场景分解
    ↓
未来: 占据 + 流 + 语义 的统一高效表示
```

**核心要点回顾**：

1. **占据网络的价值**：用密集体素取代稀疏检测框，能表示任意形状的障碍物
2. **技术路线**：3D Query (精度优先) vs BEV Lift (速度优先)
3. **占据流**：预测障碍物的未来运动，是安全规划的关键输入
4. **效率瓶颈**：全 3D 表示计算量巨大，稀疏化/降维是工程重点
5. **端到端集成**：占据预测作为规划器的安全约束层，拒绝碰撞轨迹

---

**关联阅读**：

- ⬅️ [29. BEV 感知](./29_BEV_Perception.md) — BEV 表示是占据网络的基础
- ⬅️ [30. UniAD](./30_UniAD.md) — 使用 OccFormer 的统一框架
- ➡️ [32. 端到端规划](./32_E2E_Planning.md) — 占据如何服务于轨迹规划
- ➡️ [34. 驾驶世界模型](./34_Driving_World_Models.md) — 从占据预测到世界模型

[⬅️ 返回 AD 目录](./README.md)
