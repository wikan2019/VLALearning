# 32. 端到端规划：从感知到控制的直通路径

[⬅️ 返回 AD 目录](./README.md)

---

## 1. 规划问题的定义

### 输入与输出

自动驾驶规划（Planning）的核心任务：给定**当前感知**和**目标意图**，输出一条**安全、舒适、高效**的未来轨迹。

```
输入:                                    输出:
┌────────────────────────────┐          ┌──────────────────────────────┐
│ 传感器数据                  │          │ 选项 A: 未来轨迹              │
│  · 环视相机 (6-8 cameras)  │          │  [(x₁,y₁), (x₂,y₂), ...]   │
│  · LiDAR 点云              │  Planning │  通常 3s, 每 0.5s 一个点      │
│  · 毫米波雷达              │ ───────→ │                              │
│ 自车状态                    │          │ 选项 B: 直接控制信号          │
│  · 速度、加速度、航向角     │          │  (steering, throttle, brake) │
│ 导航目标                    │          │  实时 10-36 Hz               │
│  · 高精地图路线 / 导航指令  │          └──────────────────────────────┘
└────────────────────────────┘
```

### 三大要求

| 要求 | 含义 | 度量示例 |
|------|------|---------|
| **安全** | 不与障碍物碰撞 | Collision Rate ↓ |
| **舒适** | 加速度/曲率平滑 | Jerk, Lateral Accel ↓ |
| **高效** | 朝目标持续前进 | Progress (m/s) ↑ |

### 数学形式化

轨迹优化可表述为约束优化问题：

$$\tau^* = \arg\min_{\tau} \underbrace{\mathcal{L}_{\text{imitation}}(\tau, \tau_{\text{expert}})}_{\text{模仿损失}} + \lambda_1 \underbrace{\mathcal{L}_{\text{collision}}(\tau, \text{Occ})}_{\text{碰撞惩罚}} + \lambda_2 \underbrace{\mathcal{L}_{\text{smooth}}(\tau)}_{\text{舒适度}}$$

其中轨迹 $\tau = \{(x_t, y_t)\}_{t=1}^{T}$，$T$ 为规划时域（通常 3 秒，6 个点）。

- **模仿损失**：$\mathcal{L}_{\text{imitation}} = \sum_{t} \|(x_t, y_t) - (x_t^{\text{gt}}, y_t^{\text{gt}})\|^2$
- **碰撞惩罚**：轨迹点落入占据区域的代价
- **平滑正则**：$\mathcal{L}_{\text{smooth}} = \sum_t \|\ddot{\tau}_t\|^2$（加速度的平方和）

---

## 2. 从规则规划到学习规划

### (a) 传统规则规划器

经典自动驾驶采用分阶段的**路径搜索 + 速度优化**管线：

```
感知输出                     规划管线                          控制执行
┌──────────┐    ┌─────────────────────────────────┐    ┌──────────┐
│ 障碍物列表 │ →  │ ① 路径搜索 (A*/Lattice/RRT)      │ →  │ PID/MPC  │
│ 车道线     │    │ ② 速度规划 (S-T Graph / QP)      │    │ 控制器   │
│ 交通灯     │    │ ③ 碰撞检测 → 不通过 → 回到 ①     │    │          │
└──────────┘    └─────────────────────────────────┘    └──────────┘
```

**常见搜索算法**：

| 算法 | 原理 | 优势 | 劣势 |
|------|------|------|------|
| A* | 启发式图搜索 | 最优解 | 网格分辨率限制连续性 |
| Lattice | 预生成轨迹族采样 | 动力学可行 | 覆盖有限 |
| RRT/RRT* | 随机树扩展 | 任意空间适用 | 非确定性，实时性差 |

**核心局限**：

1. **手工代价函数**：权重需逐场景调优（跟车距离、变道激进度……），无法穷尽
2. **模块间信息丢失**：感知输出的离散目标列表丢弃了大量上下文信息
3. **交互建模困难**：规则难以编码"对方会让还是不让"的博弈关系

```
规则规划器的困境:

场景: 窄路会车               手工规则:
                              IF 对向车 AND 路宽 < 阈值:
   ←🚗    🚙→                   减速等待
   ═══════════                ELSE:
                                保持车速
                              → 但阈值设多少？对方也在减速怎么办？
                                 → 规则爆炸 (combinatorial explosion)
```

### (b) 学习规划器

用**神经网络**替代手工规则，从海量人类驾驶数据中学习代价函数或直接输出轨迹。

```
传统规划:  感知结果 → 手工代价函数 → 优化器 → 轨迹
                       ↑ 人类设计

学习规划:  传感器数据 → 神经网络 → 轨迹
                       ↑ 从数据学习
```

**两种学习范式**：

| 范式 | 方式 | 代表 |
|------|------|------|
| **学习代价函数** | 网络输出 cost map, 仍用优化器搜索 | NMP, DIPP |
| **直接回归轨迹** | 网络端到端输出轨迹点 | UniAD, VAD, GenAD |

**核心优势**：

- 从数据中隐式学习交互模式——无需显式博弈建模
- 感知与规划共享特征——减少信息丢失
- 可端到端训练——梯度从规划目标反传到感知

---

## 3. 代表模型详解

### (a) VAD — Vectorized Autonomous Driving (2023)

**核心创新**：用**向量化表示**替代栅格化 BEV，更高效地建模场景。

```
传统方法:                            VAD:
┌─────────────────┐                ┌─────────────────────┐
│ 栅格 BEV Map    │                │ 向量化表示            │
│ ██░░██░░██░░██  │                │                     │
│ ░░██░░██░░██░░  │                │  Agent tokens:      │
│ ██░░██░░██░░██  │   → 替换为 →    │    🚗(x,y,θ,v)     │
│ (H×W×C 像素级)  │                │    🚙(x,y,θ,v)     │
│ 存储大、冗余多   │                │  Map tokens:        │
└─────────────────┘                │    ─── (polyline)   │
                                   │    ╌╌╌ (polyline)   │
                                   │  紧凑、灵活、可扩展  │
                                   └─────────────────────┘
```

**架构流程**：

```
环视相机 → BEV Encoder → BEV 特征
                            ↓
              ┌─────────────────────────────┐
              │    Vectorized Scene Tokens   │
              │  ┌─────────┐  ┌──────────┐  │
              │  │ Agent    │  │ Map      │  │
              │  │ Tokens   │  │ Tokens   │  │
              │  └────┬─────┘  └────┬─────┘  │
              │       └──────┬──────┘        │
              │         Interaction          │
              │         Attention            │
              └──────────┬──────────────────┘
                         ↓
              ┌──────────────────────┐
              │ Ego Planning Query   │
              │ + Navigation Command │
              │         ↓            │
              │  Trajectory Output   │
              │  [(x₁,y₁),...,(x₆,y₆)] │
              └──────────────────────┘
```

**关键设计**：

| 模块 | 说明 |
|------|------|
| **Vectorized Map** | 车道线/道路边界用 polyline 序列表示，非像素级栅格 |
| **Agent Interaction** | Agent token 之间做 self-attention 学习交互 |
| **Map-Agent Interaction** | Agent 与 Map token 之间做 cross-attention，建模拓扑关系 |
| **Planning Head** | Ego query 融合场景信息后直接回归 6 个轨迹点 |

**VAD vs UniAD**：

| 维度 | UniAD | VAD |
|------|-------|-----|
| 场景表示 | 稠密 BEV 栅格 | **向量化 token** |
| 任务数量 | 6 个子任务 | **聚焦规划** |
| 推理速度 | ~2 FPS | **~5 FPS** |
| L2 误差 (3s) | 1.03 m | **0.82 m** |
| 碰撞率 | 0.31% | **0.07%** |

### (b) GenAD — Generative End-to-End AD (2024)

**核心思想**：将驾驶视为**未来生成问题**——不仅预测自车轨迹，同时生成所有交通参与者的未来运动。

```
GenAD 的核心洞察:

  未来不是确定的！同一场景下有多种合理的未来:

  场景: 前方十字路口
  ┌──────────────────┐
  │         │        │
  │    ╱────┼────╲   │    可能的未来:
  │   ╱     │     ╲  │    ① 直行 ─→
  │  ╱      │      ╲ │    ② 左转 ╱
  │─────────┼────────│    ③ 右转 ╲
  │         │        │
  │       🚗        │    → 需要建模 multimodal 分布！
  └──────────────────┘
```

**架构核心**：

```
环视相机 → BEV Features
              ↓
   Instance-Centric Scene Tokenizer
   ┌────────────────────────────┐
   │  每个 agent 以自身为中心     │
   │  编码局部上下文 → token      │
   └─────────────┬──────────────┘
                 ↓
   Structural VAE (Variational Autoencoder)
   ┌────────────────────────────┐
   │  Encoder: 未来轨迹 → z      │   训练时使用
   │  Decoder: z → 未来轨迹       │   推理时从先验采样
   │  z ~ N(μ, σ²)              │   → 多模态分布
   └─────────────┬──────────────┘
                 ↓
   同时输出: Agent Motion + Ego Planning
```

**VAE 的多模态采样**：

$$q(z|x_{\text{past}}, x_{\text{future}}) = \mathcal{N}(\mu_\phi, \sigma_\phi^2) \quad \text{(训练时 posterior)}$$

$$p(z|x_{\text{past}}) = \mathcal{N}(\mu_\theta, \sigma_\theta^2) \quad \text{(推理时 prior)}$$

通过从 $p(z|x_{\text{past}})$ 多次采样并解码，获得多条候选轨迹，覆盖不同模态。

### (c) DriveTransformer (2025)

**核心创新**：用一个**统一 Transformer** 处理所有驾驶子任务，并展现清晰的 **Scaling Laws**。

```
DriveTransformer 统一架构:

传感器输入 → Backbone → Multi-scale Features
                            ↓
             ┌──────────────────────────────┐
             │     Shared Transformer       │
             │                              │
             │  [DET]  检测 tokens           │
             │  [MAP]  地图 tokens           │
             │  [TRK]  跟踪 tokens           │
             │  [MOT]  预测 tokens           │
             │  [PLN]  规划 tokens           │
             │         ↕                    │
             │    Shared Self-Attention     │
             │    (所有 token 交互)           │
             └──────────────────────────────┘
                   ↓        ↓        ↓
              检测结果   地图结果   规划轨迹
```

**Scaling Laws**——更大的模型 = 更好的驾驶：

```
规划 L2 误差
(m)  ▲
1.2  │ ●
1.0  │    ●
0.8  │       ●
0.6  │           ●
0.4  │               ●
0.2  │                    ●
     └──────────────────────→ 模型参数量
     10M  30M  100M 300M 1B   (log scale)

→ 端到端驾驶模型首次展示类似 LLM 的 scaling law
```

| 特性 | 说明 |
|------|------|
| 架构 | Task-specific tokens + Shared attention |
| Scaling | 从 10M→1B 参数，性能持续提升 |
| 效率 | Token 化设计避免冗余计算 |
| SOTA | nuScenes 规划基准新 SOTA (2025) |

### (d) UniUGP — Unify Understanding + Generation + Planning (2025)

**核心思想**：统一**理解**（场景描述）、**生成**（未来视频）、**规划**（轨迹输出）三大能力。

```
UniUGP 三合一范式:

┌─────────────────────────────────────────────────┐
│                 UniUGP                           │
│                                                 │
│  ① Understanding (理解)                          │
│     "前方红灯，左侧有行人正在过马路"                │
│            ↓ Chain-of-Thought                    │
│  ② Generation (生成)                              │
│     预测未来 2 秒的驾驶视频                         │
│     [frame_t+1] [frame_t+2] [frame_t+3] ...     │
│            ↓ 物理一致性约束                        │
│  ③ Planning (规划)                                │
│     输出安全轨迹: [(x₁,y₁), ..., (x₆,y₆)]       │
│                                                 │
└─────────────────────────────────────────────────┘
```

**Chain-of-Thought 驾驶推理**——类似人类驾驶员的思考过程：

```
输入: 环视图像 + "直行通过十字路口"

思维链:
  Step 1: 场景理解 → "前方信号灯为绿灯，右侧有一辆车正在减速"
  Step 2: 风险评估 → "右侧车辆可能要右转进入我的车道"
  Step 3: 决策推理 → "保持车速但预备减速，密切关注右侧车辆"
  Step 4: 轨迹生成 → [(0,2), (0,4), (0,5.5), (0.1,7), ...]
```

**三种能力的协同**：

| 能力 | 作用 | 技术手段 |
|------|------|---------|
| Understanding | 场景语义理解、风险识别 | VLM + Chain-of-Thought |
| Generation | 预演未来，确保物理一致性 | Video Diffusion Model |
| Planning | 输出可执行轨迹 | Trajectory Decoder |

---

## 4. 生成式规划 vs 判别式规划

这是端到端规划领域的核心技术分歧——与 VLA 领域面临**完全相同的多模态分布问题**。

### 判别式规划 (Discriminative Planning)

直接回归**单一最优轨迹**：

$$\tau^* = f_\theta(\text{sensor data})$$

- 训练目标：最小化与专家轨迹的 L2 距离
- 问题：**mode averaging**——当未来存在多种合理选择时，回归均值会给出一条不合理的"折中"轨迹

```
Mode Averaging 问题:

场景: 前方有障碍物，可左绕或右绕

  左绕轨迹: ╱‾‾╲              专家数据中左绕和右绕各 50%
  右绕轨迹:      ╲__╱          L2 回归的结果？
                                       ↓
  回归均值:  ──── ●障碍物 ────    直直撞上去！（两条轨迹的均值）
             (不可行！)
```

### 生成式规划 (Generative Planning)

建模轨迹的**概率分布**，通过采样获得多条候选：

$$p_\theta(\tau | \text{sensor data}) \quad \text{→ 采样} \quad \tau_1, \tau_2, \ldots, \tau_K$$

| 生成方法 | 原理 | 代表工作 |
|---------|------|---------|
| **VAE** | 学习隐空间分布，解码出多条轨迹 | GenAD |
| **Diffusion** | 从噪声逐步去噪恢复轨迹 | CTG++, DiffusionDrive |
| **Flow Matching** | 学习噪声→轨迹的确定性 ODE 流 | FlowDrive |
| **Autoregressive** | 逐点生成轨迹序列 | MotionLM |

### 对比总结

| 维度 | 判别式 | 生成式 |
|------|--------|--------|
| 输出 | 单条轨迹 | 轨迹分布 (多条候选) |
| 多模态处理 | 差 (mode averaging) | **好** (显式多模态) |
| 训练稳定性 | 高 | 中 (需调生成参数) |
| 推理速度 | **快** (单次前向) | 慢 (需多步去噪/采样) |
| 安全选择 | 无法比较 | 可从候选中选最安全 |
| 与 VLA 的关联 | RT-1 式回归 | π₀ / Diffusion Policy |

### 与 VLA 的共性

```
VLA 领域:                           自动驾驶规划:
  动作多模态问题                       轨迹多模态问题
  (同一场景，多种合理抓取方式)          (同一场景，多种合理行驶路线)
       ↓                                  ↓
  解决方案: Flow Matching /             解决方案: VAE / Diffusion /
           Diffusion Policy                     Flow Matching
       ↓                                  ↓
  代表: π₀, Octo                       代表: GenAD, DiffusionDrive
```

> **关键洞察**：端到端驾驶规划和机器人 VLA **面临完全相同的技术挑战**——连解决方案都是同源的。这也是自动驾驶 E2E 技术和 VLA 技术正在加速融合的根本原因。

---

## 5. 开放挑战

### (1) 长尾场景 (Long-tail Scenarios)

```
频率 ▲
     │████████████  正常行驶 (99%)
     │██
     │█
     │▌ 施工区域
     │▎ 逆行车辆
     │▏ 动物横穿
     │  翻覆卡车
     └───────────────────────→ 场景类型
      常见           罕见但致命
```

训练数据中的长尾分布意味着模型对罕见场景**几乎没有学习样本**。解决方向：

- **数据增强**：用世界模型 / 仿真器生成罕见场景
- **主动学习**：挖掘真实数据中的困难样本
- **规则兜底**：在神经网络之外保留安全规则作为最后防线

### (2) 可解释性 (Interpretability)

端到端模型是**黑箱**——无法回答"为什么这样规划"。

| 方法 | 思路 | 代表 |
|------|------|------|
| Attention 可视化 | 展示模型关注的区域 | UniAD |
| 语言解释 | 用 VLM 生成决策理由 | EMMA, DriveVLM |
| Chain-of-Thought | 显式推理链 | UniUGP |
| 中间表示 | 输出检测/预测中间结果 | VAD, DriveTransformer |

### (3) 闭环验证 (Closed-loop Validation)

```
开环评估 (Open-loop):              闭环评估 (Closed-loop):
┌──────────────────────┐          ┌──────────────────────────┐
│ 回放数据集             │          │ 仿真器 / 真实车辆          │
│ 模型预测轨迹           │          │ 模型输出 → 执行 → 环境变化  │
│ vs 人类真实轨迹        │          │       ↑ 反馈 ↓             │
│ 计算 L2 距离           │          │   新的观测 → 再预测         │
│                      │          │ 评估: 碰撞？违规？到达？    │
│ 问题: 不反映真实驾驶   │          │ 更真实，但成本高            │
└──────────────────────┘          └──────────────────────────┘
```

开环指标（L2 误差）与真实驾驶能力**相关性很弱**——低 L2 不代表安全，高 L2 不代表危险。闭环验证是评估端到端模型的黄金标准，但构建高保真仿真环境成本极高。

### (4) 实时部署 (Real-time Deployment)

| 模型 | 推理速度 | 平台 | 可部署? |
|------|---------|------|---------|
| UniAD | ~2 FPS | A100 | 否 |
| VAD | ~5 FPS | A100 | 勉强 |
| DriveTransformer | ~8 FPS | A100 | 需优化 |
| FlashOcc + 轻量规划 | >20 FPS | Orin | **是** |
| Tesla FSD v12 | 36 Hz | HW4 (自研) | **是** |

车载芯片（NVIDIA Orin, Tesla HW4）的算力远低于数据中心 GPU，模型必须经过**量化、蒸馏、算子优化**才能满足实时性要求（>10 Hz）。

---

## 6. 总结与展望

```
演进路径:

规则规划 (A*/Lattice)             ← 手工设计，泛化差
    ↓
学习代价函数 (NMP, 2019)          ← 用数据学 cost，仍需优化器
    ↓
端到端轨迹回归 (UniAD, 2023)      ← 统一框架，但判别式
    ↓
向量化规划 (VAD, 2023)            ← 更高效的场景表示
    ↓
生成式规划 (GenAD, 2024)          ← 多模态分布建模
    ↓
统一缩放 (DriveTransformer, 2025) ← Scaling Laws
    ↓
理解+生成+规划 (UniUGP, 2025)     ← 三合一，Chain-of-Thought
    ↓
未来: 闭环 RL + 世界模型 + 安全保障
```

**核心要点回顾**：

1. **规划是端到端驾驶的终极目标**：所有感知、预测模块最终服务于输出安全轨迹
2. **向量化 > 栅格化**：VAD 证明了向量化场景表示更高效灵活
3. **生成式 > 判别式**：多模态未来需要分布建模，与 VLA 同源
4. **Scaling Laws 成立**：DriveTransformer 首次证明端到端驾驶模型遵循缩放定律
5. **三合一是趋势**：理解 + 生成 + 规划的统一框架正在成为主流方向

---

**关联阅读**：

- ⬅️ [31. 占据网络](./31_Occupancy_Networks.md) — 占据预测作为规划的安全约束
- ⬅️ [30. UniAD](./30_UniAD.md) — 首个统一感知-规划框架
- ➡️ [33. VLM/VLA 驾驶](./33_VLM_Driving.md) — 大模型为规划注入常识推理
- ➡️ [34. 驾驶世界模型](./34_Driving_World_Models.md) — 世界模型为规划提供未来预演
- ↔️ [VLA/24. 世界模型与规划](../VLA_Advances/24_World_Models_Planning.md) — VLA 领域的同源技术

[⬅️ 返回 AD 目录](./README.md)
